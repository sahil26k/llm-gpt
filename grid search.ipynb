{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e47f75d-cbf3-428b-a7c2-799aacc92560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Vocabulary size: 32171\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 256, 4, 6)\n",
      "step: 0, train loss: 10.4306001663208, val loss: 10.430209159851074\n",
      "step: 50, train loss: 3.2726123332977295, val loss: 3.2111847400665283\n",
      "step: 100, train loss: 2.97087025642395, val loss: 3.057042121887207\n",
      "step: 150, train loss: 2.839160203933716, val loss: 2.829014539718628\n",
      "Final loss: 2.6228508949279785\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 256, 4, 10)\n",
      "step: 0, train loss: 10.437643051147461, val loss: 10.440773963928223\n",
      "step: 50, train loss: 2.910841464996338, val loss: 2.8277034759521484\n",
      "step: 100, train loss: 2.800921678543091, val loss: 2.7220914363861084\n",
      "step: 150, train loss: 2.691829204559326, val loss: 2.7112538814544678\n",
      "Final loss: 2.7108919620513916\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 256, 4, 12)\n",
      "step: 0, train loss: 10.391509056091309, val loss: 10.391407012939453\n",
      "step: 50, train loss: 2.9284470081329346, val loss: 2.826960802078247\n",
      "step: 100, train loss: 2.6976218223571777, val loss: 2.731945276260376\n",
      "step: 150, train loss: 2.6433606147766113, val loss: 2.845752000808716\n",
      "Final loss: 2.544109344482422\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 256, 8, 6)\n",
      "step: 0, train loss: 10.415820121765137, val loss: 10.419646263122559\n",
      "step: 50, train loss: 2.8289988040924072, val loss: 2.822334051132202\n",
      "step: 100, train loss: 2.707937240600586, val loss: 2.740169048309326\n",
      "step: 150, train loss: 2.762730598449707, val loss: 2.6743338108062744\n",
      "Final loss: 2.791827440261841\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 256, 8, 10)\n",
      "step: 0, train loss: 10.457076072692871, val loss: 10.456543922424316\n",
      "step: 50, train loss: 2.9598865509033203, val loss: 2.962195634841919\n",
      "step: 100, train loss: 2.7975244522094727, val loss: 2.7468607425689697\n",
      "step: 150, train loss: 2.6803057193756104, val loss: 2.9055135250091553\n",
      "Final loss: 2.720675230026245\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 256, 8, 12)\n",
      "step: 0, train loss: 10.393097877502441, val loss: 10.39538288116455\n",
      "step: 50, train loss: 3.2097442150115967, val loss: 3.191380500793457\n",
      "step: 100, train loss: 3.154862880706787, val loss: 3.1114413738250732\n",
      "step: 150, train loss: 3.1631882190704346, val loss: 3.2536373138427734\n",
      "Final loss: 3.197814464569092\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 256, 12, 6)\n",
      "step: 0, train loss: 10.469804763793945, val loss: 10.469200134277344\n",
      "step: 50, train loss: 2.7742364406585693, val loss: 2.841243267059326\n",
      "step: 100, train loss: 2.6920297145843506, val loss: 2.667670249938965\n",
      "step: 150, train loss: 2.6279635429382324, val loss: 2.637524366378784\n",
      "Final loss: 2.5791103839874268\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 256, 12, 10)\n",
      "step: 0, train loss: 10.3956880569458, val loss: 10.394527435302734\n",
      "step: 50, train loss: 2.8265674114227295, val loss: 2.8069264888763428\n",
      "step: 100, train loss: 2.743509531021118, val loss: 2.751025438308716\n",
      "step: 150, train loss: 2.8360726833343506, val loss: 2.6685421466827393\n",
      "Final loss: 2.5209622383117676\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 256, 12, 12)\n",
      "step: 0, train loss: 10.422690391540527, val loss: 10.423853874206543\n",
      "step: 50, train loss: 3.2413384914398193, val loss: 3.203916549682617\n",
      "step: 100, train loss: 3.1979095935821533, val loss: 3.4158589839935303\n",
      "step: 150, train loss: 3.2184274196624756, val loss: 3.315675735473633\n",
      "Final loss: 3.2653300762176514\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 384, 4, 6)\n",
      "step: 0, train loss: 10.4520263671875, val loss: 10.456812858581543\n",
      "step: 50, train loss: 2.74824595451355, val loss: 2.779933452606201\n",
      "step: 100, train loss: 2.6911633014678955, val loss: 2.678971767425537\n",
      "step: 150, train loss: 2.69541072845459, val loss: 2.700976848602295\n",
      "Final loss: 2.7404255867004395\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 384, 4, 10)\n",
      "step: 0, train loss: 10.415931701660156, val loss: 10.416529655456543\n",
      "step: 50, train loss: 3.232520341873169, val loss: 3.2843198776245117\n",
      "step: 100, train loss: 3.2388837337493896, val loss: 3.2003591060638428\n",
      "step: 150, train loss: 3.343625783920288, val loss: 3.2254035472869873\n",
      "Final loss: 3.2679383754730225\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 384, 4, 12)\n",
      "step: 0, train loss: 10.493045806884766, val loss: 10.491695404052734\n",
      "step: 50, train loss: 3.293349027633667, val loss: 3.272935390472412\n",
      "step: 100, train loss: 3.2614219188690186, val loss: 3.2638585567474365\n",
      "step: 150, train loss: 3.2460203170776367, val loss: 3.219442129135132\n",
      "Final loss: 3.189321517944336\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 384, 8, 6)\n",
      "step: 0, train loss: 10.435046195983887, val loss: 10.434676170349121\n",
      "step: 50, train loss: 3.2500925064086914, val loss: 3.2694900035858154\n",
      "step: 100, train loss: 3.109961748123169, val loss: 3.092921495437622\n",
      "step: 150, train loss: 2.921661376953125, val loss: 2.990046977996826\n",
      "Final loss: 2.8502614498138428\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 384, 8, 10)\n",
      "step: 0, train loss: 10.443709373474121, val loss: 10.44247817993164\n",
      "step: 50, train loss: 3.2337677478790283, val loss: 3.263718605041504\n",
      "step: 100, train loss: 3.2166013717651367, val loss: 3.258237838745117\n",
      "step: 150, train loss: 3.2860662937164307, val loss: 3.227290153503418\n",
      "Final loss: 3.1234219074249268\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 384, 8, 12)\n",
      "step: 0, train loss: 10.475711822509766, val loss: 10.47352409362793\n",
      "step: 50, train loss: 3.2333626747131348, val loss: 3.2194488048553467\n",
      "step: 100, train loss: 3.2323358058929443, val loss: 3.227071762084961\n",
      "step: 150, train loss: 3.2036166191101074, val loss: 3.2147064208984375\n",
      "Final loss: 3.1631662845611572\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 384, 12, 6)\n",
      "step: 0, train loss: 10.395456314086914, val loss: 10.393834114074707\n",
      "step: 50, train loss: 3.0157535076141357, val loss: 3.039288282394409\n",
      "step: 100, train loss: 3.183558464050293, val loss: 2.900500774383545\n",
      "step: 150, train loss: 2.7771453857421875, val loss: 2.7451364994049072\n",
      "Final loss: 2.6708710193634033\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 384, 12, 10)\n",
      "step: 0, train loss: 10.382384300231934, val loss: 10.387526512145996\n",
      "step: 50, train loss: 3.254779577255249, val loss: 3.2476110458374023\n",
      "step: 100, train loss: 3.2183802127838135, val loss: 3.2189998626708984\n",
      "step: 150, train loss: 3.239011287689209, val loss: 3.203967809677124\n",
      "Final loss: 3.172049045562744\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 384, 12, 12)\n",
      "step: 0, train loss: 10.495636940002441, val loss: 10.4962739944458\n",
      "step: 50, train loss: 3.331663131713867, val loss: 3.227792978286743\n",
      "step: 100, train loss: 3.2917211055755615, val loss: 3.238804578781128\n",
      "step: 150, train loss: 3.260183811187744, val loss: 3.236551523208618\n",
      "Final loss: 3.2250773906707764\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 512, 4, 6)\n",
      "step: 0, train loss: 10.484999656677246, val loss: 10.48709487915039\n",
      "step: 50, train loss: 3.2178869247436523, val loss: 3.2565174102783203\n",
      "step: 100, train loss: 3.2769381999969482, val loss: 3.2636263370513916\n",
      "step: 150, train loss: 3.253187894821167, val loss: 3.321049690246582\n",
      "Final loss: 3.195065498352051\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 512, 4, 10)\n",
      "step: 0, train loss: 10.458649635314941, val loss: 10.45146369934082\n",
      "step: 50, train loss: 3.309725284576416, val loss: 3.241286516189575\n",
      "step: 100, train loss: 3.2202179431915283, val loss: 3.2200634479522705\n",
      "step: 150, train loss: 3.216794967651367, val loss: 3.2224624156951904\n",
      "Final loss: 3.045184373855591\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 512, 4, 12)\n",
      "step: 0, train loss: 10.541452407836914, val loss: 10.539261817932129\n",
      "step: 50, train loss: 3.3452377319335938, val loss: 3.3258392810821533\n",
      "step: 100, train loss: 3.2400851249694824, val loss: 3.2895421981811523\n",
      "step: 150, train loss: 3.2290658950805664, val loss: 3.243840217590332\n",
      "Final loss: 3.6104393005371094\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 512, 8, 6)\n",
      "step: 0, train loss: 10.443927764892578, val loss: 10.439640998840332\n",
      "step: 50, train loss: 2.8345911502838135, val loss: 2.8625800609588623\n",
      "step: 100, train loss: 2.7662012577056885, val loss: 2.772447109222412\n",
      "step: 150, train loss: 2.7415239810943604, val loss: 2.727327346801758\n",
      "Final loss: 2.8283441066741943\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 512, 8, 10)\n",
      "step: 0, train loss: 10.375317573547363, val loss: 10.375219345092773\n",
      "step: 50, train loss: 3.218928813934326, val loss: 3.219590425491333\n",
      "step: 100, train loss: 3.2142889499664307, val loss: 3.194396734237671\n",
      "step: 150, train loss: 3.3088629245758057, val loss: 3.1842076778411865\n",
      "Final loss: 3.416456460952759\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 512, 8, 12)\n",
      "step: 0, train loss: 10.500327110290527, val loss: 10.504899978637695\n",
      "step: 50, train loss: 3.3013851642608643, val loss: 3.235440731048584\n",
      "step: 100, train loss: 3.2077019214630127, val loss: 3.1883792877197266\n",
      "step: 150, train loss: 3.307288408279419, val loss: 3.2719831466674805\n",
      "Final loss: 3.1886038780212402\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 512, 12, 6)\n",
      "step: 0, train loss: 10.525810241699219, val loss: 10.52340316772461\n",
      "step: 50, train loss: 3.4805426597595215, val loss: 3.427943706512451\n",
      "step: 100, train loss: 3.256603956222534, val loss: 3.1975290775299072\n",
      "step: 150, train loss: 3.2239890098571777, val loss: 3.257600784301758\n",
      "Final loss: 3.0440056324005127\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 512, 12, 10)\n",
      "step: 0, train loss: 10.448741912841797, val loss: 10.443078994750977\n",
      "step: 50, train loss: 3.249713182449341, val loss: 3.2728700637817383\n",
      "step: 100, train loss: 3.231513261795044, val loss: 3.264301061630249\n",
      "step: 150, train loss: 3.2758138179779053, val loss: 3.2264907360076904\n",
      "Final loss: 3.095538377761841\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 512, 12, 12)\n",
      "step: 0, train loss: 10.454791069030762, val loss: 10.458603858947754\n",
      "step: 50, train loss: 3.2854340076446533, val loss: 3.23403263092041\n",
      "step: 100, train loss: 3.2333145141601562, val loss: 3.227069616317749\n",
      "step: 150, train loss: 3.3690524101257324, val loss: 3.2824981212615967\n",
      "Final loss: 3.2314705848693848\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 256, 4, 6)\n",
      "step: 0, train loss: 10.400988578796387, val loss: 10.400313377380371\n",
      "step: 50, train loss: 6.895771503448486, val loss: 6.897395610809326\n",
      "step: 100, train loss: 4.475862979888916, val loss: 4.4678144454956055\n",
      "step: 150, train loss: 3.382486581802368, val loss: 3.3965070247650146\n",
      "Final loss: 3.153686761856079\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 256, 4, 10)\n",
      "step: 0, train loss: 10.39498519897461, val loss: 10.393898963928223\n",
      "step: 50, train loss: 6.861912727355957, val loss: 6.875483989715576\n",
      "step: 100, train loss: 4.533825874328613, val loss: 4.521502494812012\n",
      "step: 150, train loss: 3.424802541732788, val loss: 3.4243111610412598\n",
      "Final loss: 3.0905933380126953\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 256, 4, 12)\n",
      "step: 0, train loss: 10.429337501525879, val loss: 10.434391021728516\n",
      "step: 50, train loss: 6.786653995513916, val loss: 6.814945220947266\n",
      "step: 100, train loss: 4.533570766448975, val loss: 4.4730024337768555\n",
      "step: 150, train loss: 3.404808759689331, val loss: 3.4306681156158447\n",
      "Final loss: 2.9659619331359863\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 256, 8, 6)\n",
      "step: 0, train loss: 10.439737319946289, val loss: 10.437100410461426\n",
      "step: 50, train loss: 6.952149868011475, val loss: 6.929983615875244\n",
      "step: 100, train loss: 4.521659851074219, val loss: 4.541517734527588\n",
      "step: 150, train loss: 3.457747459411621, val loss: 3.3779046535491943\n",
      "Final loss: 3.306539535522461\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 256, 8, 10)\n",
      "step: 0, train loss: 10.439727783203125, val loss: 10.440703392028809\n",
      "step: 50, train loss: 6.897068977355957, val loss: 6.891545295715332\n",
      "step: 100, train loss: 4.521303653717041, val loss: 4.545743465423584\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 216\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    215\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 216\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    218\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[4], line 118\u001b[0m, in \u001b[0;36mcreate_model.<locals>.GptLanguageModel.forward\u001b[1;34m(self, index, targets)\u001b[0m\n\u001b[0;32m    115\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[0;32m    117\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb  \u001b[38;5;66;03m# b,t,c\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[0;32m    120\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 86\u001b[0m, in \u001b[0;36mcreate_model.<locals>.Block.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 86\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x \u001b[38;5;241m+\u001b[39m y)\n\u001b[0;32m     88\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffws(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 59\u001b[0m, in \u001b[0;36mcreate_model.<locals>.MultiHeadAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 59\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     60\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out))\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 44\u001b[0m, in \u001b[0;36mcreate_model.<locals>.Head.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(x)\n\u001b[0;32m     43\u001b[0m wei \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m---> 44\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[43mwei\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtril\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-inf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m wei \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(wei, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     46\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(wei)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# Hyperparameter grid\n",
    "\n",
    "hyperparameter_grid = {\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'block_size': [64, 128, 256],\n",
    "    'max_iters': [200],\n",
    "    'learning_rate': [1e-3, 1e-4, 1e-5],\n",
    "    'n_embd': [256, 384, 512],\n",
    "    'n_head': [4, 8, 12],\n",
    "    'n_layer': [6, 10, 12],\n",
    "}\n",
    "dropout= 0.2\n",
    "eval_iters= 50\n",
    "# Function to create model with given hyperparameters\n",
    "def create_model(vocab_size, n_embd, n_head, n_layer):\n",
    "    class Head(nn.Module):\n",
    "        def __init__(self, head_size):\n",
    "            super().__init__()\n",
    "            self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            B, T, C = x.shape\n",
    "            k = self.key(x)\n",
    "            q = self.query(x)\n",
    "            wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "            wei = F.softmax(wei, dim=-1)\n",
    "            wei = self.dropout(wei)\n",
    "            v = self.value(x)\n",
    "            out = wei @ v\n",
    "            return out\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        def __init__(self, num_heads, head_size):\n",
    "            super().__init__()\n",
    "            self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "            self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "            out = self.dropout(self.proj(out))\n",
    "            return out\n",
    "\n",
    "    class FeedForward(nn.Module):\n",
    "        def __init__(self, n_embd):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_embd, 4 * n_embd),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4 * n_embd, n_embd),\n",
    "                nn.Dropout(dropout),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    class Block(nn.Module):\n",
    "        def __init__(self, n_embd, n_head):\n",
    "            super().__init__()\n",
    "            head_size = n_embd // n_head\n",
    "            self.sa = MultiHeadAttention(n_head, head_size)\n",
    "            self.ffws = FeedForward(n_embd)\n",
    "            self.ln1 = nn.LayerNorm(n_embd)\n",
    "            self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        def forward(self, x):\n",
    "            y = self.sa(x)\n",
    "            x = self.ln1(x + y)\n",
    "            y = self.ffws(x)\n",
    "            x = self.ln2(x + y)\n",
    "            return x\n",
    "\n",
    "    class GptLanguageModel(nn.Module):\n",
    "        def __init__(self, vocab_size):\n",
    "            super().__init__()\n",
    "            self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "            self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "            self.block = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "            self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
    "            self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "            self.apply(self._init_weights)\n",
    "\n",
    "        def _init_weights(self, module):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        def forward(self, index, targets=None):\n",
    "            B, T = index.shape\n",
    "\n",
    "            tok_emb = self.token_embedding_table(index)\n",
    "            pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "\n",
    "            x = tok_emb + pos_emb  # b,t,c\n",
    "            x = self.block(x)\n",
    "            x = self.ln_f(x)\n",
    "            logits = self.lm_head(x)\n",
    "\n",
    "            if targets is None:\n",
    "                loss = None\n",
    "            else:\n",
    "                B, T, C = logits.shape\n",
    "                logits = logits.view(B * T, C)\n",
    "                targets = targets.view(B * T)\n",
    "                loss = F.cross_entropy(logits, targets)\n",
    "            return logits, loss\n",
    "\n",
    "        def generate(self, index, max_new_tokens):\n",
    "            for _ in range(max_new_tokens):\n",
    "                logits, _ = self.forward(index)\n",
    "                logits = logits[:, -1, :]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                index_next = torch.multinomial(probs, num_samples=1)\n",
    "                index = torch.cat((index, index_next), dim=1)\n",
    "            return index\n",
    "\n",
    "    return GptLanguageModel(vocab_size).to(device)\n",
    "\n",
    "# Define data functions\n",
    "def get_random_chunk(split):\n",
    "    filename = \"train_split.txt\" if split == 'train' else \"val_split.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, file_size - block_size * batch_size)\n",
    "\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size * batch_size - 1)\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "            \n",
    "    return data\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix]).to(device)\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix]).to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    return out\n",
    "\n",
    "# Load vocabulary\n",
    "chars = \"\"\n",
    "with open('vocab.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "string_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "# Initialize results\n",
    "best_loss = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Grid search over hyperparameters\n",
    "for params in itertools.product(*hyperparameter_grid.values()):\n",
    "    batch_size, block_size, max_iters, learning_rate, n_embd, n_head, n_layer = params\n",
    "    \n",
    "    print(f\"\\nTesting combination: {params}\")\n",
    "    \n",
    "    # Create model with current hyperparameters\n",
    "    model = create_model(vocab_size, n_embd, n_head, n_layer)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "        if iter % eval_iters == 0:\n",
    "            losses = estimate_loss(model, eval_iters)\n",
    "            train_losses.append(losses['train'])\n",
    "            val_losses.append(losses['val'])\n",
    "            print(f\"step: {iter}, train loss: {losses['train']}, val loss: {losses['val']}\")\n",
    "\n",
    "        xb, yb = get_batch('train')\n",
    "        logits, loss = model.forward(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Final loss: {loss.item()}\")\n",
    "    \n",
    "    # Check if the current model is the best one\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        best_params = {\n",
    "            'batch_size': batch_size,\n",
    "            'block_size': block_size,\n",
    "            'max_iters': max_iters,\n",
    "            'learning_rate': learning_rate,\n",
    "            'n_embd': n_embd,\n",
    "            'n_head': n_head,\n",
    "            'n_layer': n_layer\n",
    "        }\n",
    "\n",
    "# Save best parameters\n",
    "with open('best_params.pkl', 'wb') as f:\n",
    "    pickle.dump(best_params, f)\n",
    "\n",
    "print(f\"Best parameters: {best_params} with loss: {best_loss}\")\n",
    "\n",
    "# Optionally plot losses\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f6ac05-34ca-4182-b6b9-3323a8e565bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Vocabulary size: 32171\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 256, 4, 6)\n",
      "step: 0, train loss: 10.446089744567871, val loss: 10.447775840759277\n",
      "step: 50, train loss: 3.255851984024048, val loss: 3.2606325149536133\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 256, 4, 10)\n",
      "step: 0, train loss: 10.464710235595703, val loss: 10.464530944824219\n",
      "step: 50, train loss: 2.9358229637145996, val loss: 2.8663995265960693\n",
      "step: 100, train loss: 2.7313382625579834, val loss: 2.751823663711548\n",
      "step: 150, train loss: 2.7066102027893066, val loss: 2.6303164958953857\n",
      "Final loss: 2.55635666847229\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 256, 4, 12)\n",
      "step: 0, train loss: 10.466771125793457, val loss: 10.461652755737305\n",
      "step: 50, train loss: 2.9678618907928467, val loss: 3.0173540115356445\n",
      "step: 100, train loss: 2.8077785968780518, val loss: 2.823378086090088\n",
      "step: 150, train loss: 2.812192678451538, val loss: 2.723118305206299\n",
      "Final loss: 2.984623432159424\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 256, 8, 6)\n",
      "step: 0, train loss: 10.412134170532227, val loss: 10.411206245422363\n",
      "step: 50, train loss: 2.834810733795166, val loss: 2.7833478450775146\n",
      "step: 100, train loss: 2.6828277111053467, val loss: 2.762200355529785\n",
      "step: 150, train loss: 2.6730947494506836, val loss: 2.771859645843506\n",
      "Final loss: 3.025498628616333\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 256, 8, 10)\n",
      "step: 0, train loss: 10.430877685546875, val loss: 10.429117202758789\n",
      "step: 50, train loss: 2.8646926879882812, val loss: 2.826857566833496\n",
      "step: 100, train loss: 2.747921943664551, val loss: 2.775242805480957\n",
      "step: 150, train loss: 2.643873929977417, val loss: 2.720560312271118\n",
      "Final loss: 2.4734954833984375\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 256, 8, 12)\n",
      "step: 0, train loss: 10.476613998413086, val loss: 10.478145599365234\n",
      "step: 50, train loss: 3.187380313873291, val loss: 3.2421698570251465\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 256, 12, 6)\n",
      "step: 0, train loss: 10.43419075012207, val loss: 10.436083793640137\n",
      "step: 50, train loss: 2.9821207523345947, val loss: 2.9699060916900635\n",
      "step: 100, train loss: 2.7209527492523193, val loss: 2.9636776447296143\n",
      "step: 150, train loss: 2.6893997192382812, val loss: 2.6689321994781494\n",
      "Final loss: 2.8690619468688965\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 256, 12, 10)\n",
      "step: 0, train loss: 10.451274871826172, val loss: 10.450577735900879\n",
      "step: 50, train loss: 2.7851595878601074, val loss: 2.8620309829711914\n",
      "step: 100, train loss: 2.6974270343780518, val loss: 2.7852108478546143\n",
      "step: 150, train loss: 2.707179546356201, val loss: 2.735797166824341\n",
      "Final loss: 2.6115217208862305\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 256, 12, 12)\n",
      "step: 0, train loss: 10.448028564453125, val loss: 10.448259353637695\n",
      "step: 50, train loss: 3.2528281211853027, val loss: 3.2589240074157715\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 384, 4, 6)\n",
      "step: 0, train loss: 10.50009536743164, val loss: 10.497186660766602\n",
      "step: 50, train loss: 2.794919967651367, val loss: 2.8666818141937256\n",
      "step: 100, train loss: 2.718127727508545, val loss: 2.698721408843994\n",
      "step: 150, train loss: 2.7405734062194824, val loss: 2.6365792751312256\n",
      "Final loss: 2.4056570529937744\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 384, 4, 10)\n",
      "step: 0, train loss: 10.423541069030762, val loss: 10.423993110656738\n",
      "step: 50, train loss: 3.2341766357421875, val loss: 3.2128260135650635\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 384, 4, 12)\n",
      "step: 0, train loss: 10.437536239624023, val loss: 10.438019752502441\n",
      "step: 50, train loss: 3.262420654296875, val loss: 3.3052921295166016\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 384, 8, 6)\n",
      "step: 0, train loss: 10.441089630126953, val loss: 10.4465970993042\n",
      "step: 50, train loss: 2.818019390106201, val loss: 2.856818914413452\n",
      "step: 100, train loss: 2.6700165271759033, val loss: 2.6775777339935303\n",
      "step: 150, train loss: 2.6887922286987305, val loss: 2.7375879287719727\n",
      "Final loss: 2.579413890838623\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 384, 8, 10)\n",
      "step: 0, train loss: 10.386968612670898, val loss: 10.389405250549316\n",
      "step: 50, train loss: 3.36997127532959, val loss: 3.318319797515869\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 384, 8, 12)\n",
      "step: 0, train loss: 10.394377708435059, val loss: 10.393619537353516\n",
      "step: 50, train loss: 3.2547521591186523, val loss: 3.2635674476623535\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 384, 12, 6)\n",
      "step: 0, train loss: 10.499926567077637, val loss: 10.496635437011719\n",
      "step: 50, train loss: 2.741880416870117, val loss: 2.7245562076568604\n",
      "step: 100, train loss: 2.658562660217285, val loss: 2.835505962371826\n",
      "step: 150, train loss: 2.664245843887329, val loss: 2.643552780151367\n",
      "Final loss: 2.6287379264831543\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 384, 12, 10)\n",
      "step: 0, train loss: 10.413457870483398, val loss: 10.42194938659668\n",
      "step: 50, train loss: 3.3438761234283447, val loss: 3.2083024978637695\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 384, 12, 12)\n",
      "step: 0, train loss: 10.479915618896484, val loss: 10.4860200881958\n",
      "step: 50, train loss: 3.217700242996216, val loss: 3.301703453063965\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 512, 4, 6)\n",
      "step: 0, train loss: 10.473942756652832, val loss: 10.478545188903809\n",
      "step: 50, train loss: 3.3576743602752686, val loss: 3.3889214992523193\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 512, 4, 10)\n",
      "step: 0, train loss: 10.59799575805664, val loss: 10.594979286193848\n",
      "step: 50, train loss: 3.166294574737549, val loss: 3.238002061843872\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 512, 4, 12)\n",
      "step: 0, train loss: 10.442646026611328, val loss: 10.445879936218262\n",
      "step: 50, train loss: 3.3103301525115967, val loss: 3.254354953765869\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 512, 8, 6)\n",
      "step: 0, train loss: 10.39921760559082, val loss: 10.394457817077637\n",
      "step: 50, train loss: 3.2675769329071045, val loss: 3.2018487453460693\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 512, 8, 10)\n",
      "step: 0, train loss: 10.526679992675781, val loss: 10.522004127502441\n",
      "step: 50, train loss: 3.2587103843688965, val loss: 3.2609283924102783\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 512, 8, 12)\n",
      "step: 0, train loss: 10.487557411193848, val loss: 10.490522384643555\n",
      "step: 50, train loss: 3.249856948852539, val loss: 3.2397046089172363\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 512, 12, 6)\n",
      "step: 0, train loss: 10.445255279541016, val loss: 10.445657730102539\n",
      "step: 50, train loss: 3.194749116897583, val loss: 3.2316558361053467\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 512, 12, 10)\n",
      "step: 0, train loss: 10.552530288696289, val loss: 10.547680854797363\n",
      "step: 50, train loss: 3.219853162765503, val loss: 3.237363815307617\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.001, 512, 12, 12)\n",
      "step: 0, train loss: 10.546923637390137, val loss: 10.547558784484863\n",
      "step: 50, train loss: 3.216719627380371, val loss: 3.1871960163116455\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 256, 4, 6)\n",
      "step: 0, train loss: 10.472564697265625, val loss: 10.470785140991211\n",
      "step: 50, train loss: 6.950763702392578, val loss: 6.977203845977783\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 256, 4, 10)\n",
      "step: 0, train loss: 10.4103364944458, val loss: 10.404775619506836\n",
      "step: 50, train loss: 6.9119181632995605, val loss: 6.944671154022217\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 256, 4, 12)\n",
      "step: 0, train loss: 10.424368858337402, val loss: 10.4233980178833\n",
      "step: 50, train loss: 6.91888427734375, val loss: 6.907034873962402\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 256, 8, 6)\n",
      "step: 0, train loss: 10.417255401611328, val loss: 10.418933868408203\n",
      "step: 50, train loss: 6.933271408081055, val loss: 6.990016460418701\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 256, 8, 10)\n",
      "step: 0, train loss: 10.449175834655762, val loss: 10.447493553161621\n",
      "step: 50, train loss: 6.978639125823975, val loss: 6.965803146362305\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 256, 8, 12)\n",
      "step: 0, train loss: 10.472023010253906, val loss: 10.464033126831055\n",
      "step: 50, train loss: 6.885574817657471, val loss: 6.910354137420654\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 256, 12, 6)\n",
      "step: 0, train loss: 10.462501525878906, val loss: 10.46126937866211\n",
      "step: 50, train loss: 6.962068557739258, val loss: 6.9693403244018555\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 256, 12, 10)\n",
      "step: 0, train loss: 10.442787170410156, val loss: 10.44494342803955\n",
      "step: 50, train loss: 6.864639759063721, val loss: 6.858785629272461\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 256, 12, 12)\n",
      "step: 0, train loss: 10.456801414489746, val loss: 10.456987380981445\n",
      "step: 50, train loss: 6.92070198059082, val loss: 6.889163970947266\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 384, 4, 6)\n",
      "step: 0, train loss: 10.527670860290527, val loss: 10.526323318481445\n",
      "step: 50, train loss: 5.291558265686035, val loss: 5.24127197265625\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 384, 4, 10)\n",
      "step: 0, train loss: 10.395129203796387, val loss: 10.397966384887695\n",
      "step: 50, train loss: 5.253364086151123, val loss: 5.195327281951904\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 384, 4, 12)\n",
      "step: 0, train loss: 10.479355812072754, val loss: 10.475613594055176\n",
      "step: 50, train loss: 5.294517993927002, val loss: 5.254460334777832\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 384, 8, 6)\n",
      "step: 0, train loss: 10.480964660644531, val loss: 10.482595443725586\n",
      "step: 50, train loss: 5.398085117340088, val loss: 5.363544464111328\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 384, 8, 10)\n",
      "step: 0, train loss: 10.482285499572754, val loss: 10.47384262084961\n",
      "step: 50, train loss: 5.280582904815674, val loss: 5.271812915802002\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 384, 8, 12)\n",
      "step: 0, train loss: 10.385906219482422, val loss: 10.383191108703613\n",
      "step: 50, train loss: 5.351534843444824, val loss: 5.3273773193359375\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 384, 12, 6)\n",
      "step: 0, train loss: 10.468918800354004, val loss: 10.468924522399902\n",
      "step: 50, train loss: 5.364243984222412, val loss: 5.343441009521484\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 384, 12, 10)\n",
      "step: 0, train loss: 10.468911170959473, val loss: 10.469164848327637\n",
      "step: 50, train loss: 5.321288585662842, val loss: 5.33791446685791\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 384, 12, 12)\n",
      "step: 0, train loss: 10.450301170349121, val loss: 10.452750205993652\n",
      "step: 50, train loss: 5.216619968414307, val loss: 5.236080169677734\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 512, 4, 6)\n",
      "step: 0, train loss: 10.395463943481445, val loss: 10.395818710327148\n",
      "step: 50, train loss: 4.093304634094238, val loss: 4.11363410949707\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 512, 4, 10)\n",
      "step: 0, train loss: 10.505209922790527, val loss: 10.5010347366333\n",
      "step: 50, train loss: 4.273431777954102, val loss: 4.286320686340332\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 512, 4, 12)\n",
      "step: 0, train loss: 10.480335235595703, val loss: 10.4745512008667\n",
      "step: 50, train loss: 4.085920333862305, val loss: 4.179856777191162\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 512, 8, 6)\n",
      "step: 0, train loss: 10.512768745422363, val loss: 10.513520240783691\n",
      "step: 50, train loss: 4.133164882659912, val loss: 4.15053653717041\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 512, 8, 10)\n",
      "step: 0, train loss: 10.418266296386719, val loss: 10.41781997680664\n",
      "step: 50, train loss: 4.033189296722412, val loss: 3.9992949962615967\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 512, 8, 12)\n",
      "step: 0, train loss: 10.594574928283691, val loss: 10.593378067016602\n",
      "step: 50, train loss: 4.217922210693359, val loss: 4.158878326416016\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 512, 12, 6)\n",
      "step: 0, train loss: 10.429043769836426, val loss: 10.431419372558594\n",
      "step: 50, train loss: 4.153164863586426, val loss: 4.148726463317871\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 512, 12, 10)\n",
      "step: 0, train loss: 10.568220138549805, val loss: 10.573612213134766\n",
      "step: 50, train loss: 4.157373428344727, val loss: 4.134158134460449\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 0.0001, 512, 12, 12)\n",
      "step: 0, train loss: 10.540607452392578, val loss: 10.540185928344727\n",
      "step: 50, train loss: 4.091345310211182, val loss: 4.049532413482666\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 256, 4, 6)\n",
      "step: 0, train loss: 10.429530143737793, val loss: 10.426931381225586\n",
      "step: 50, train loss: 9.673846244812012, val loss: 9.680965423583984\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 256, 4, 10)\n",
      "step: 0, train loss: 10.458409309387207, val loss: 10.463234901428223\n",
      "step: 50, train loss: 9.418233871459961, val loss: 9.419583320617676\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 256, 4, 12)\n",
      "step: 0, train loss: 10.401681900024414, val loss: 10.39968490600586\n",
      "step: 50, train loss: 9.211751937866211, val loss: 9.185296058654785\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 256, 8, 6)\n",
      "step: 0, train loss: 10.399271011352539, val loss: 10.396056175231934\n",
      "step: 50, train loss: 9.632732391357422, val loss: 9.639440536499023\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 256, 8, 10)\n",
      "step: 0, train loss: 10.460783958435059, val loss: 10.45495891571045\n",
      "step: 50, train loss: 9.330937385559082, val loss: 9.354018211364746\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 256, 8, 12)\n",
      "step: 0, train loss: 10.458462715148926, val loss: 10.454007148742676\n",
      "step: 50, train loss: 9.218223571777344, val loss: 9.217317581176758\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 256, 12, 6)\n",
      "step: 0, train loss: 10.43386459350586, val loss: 10.433147430419922\n",
      "step: 50, train loss: 9.664978981018066, val loss: 9.672994613647461\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 256, 12, 10)\n",
      "step: 0, train loss: 10.407835960388184, val loss: 10.406899452209473\n",
      "step: 50, train loss: 9.296500205993652, val loss: 9.27566146850586\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 256, 12, 12)\n",
      "step: 0, train loss: 10.406965255737305, val loss: 10.405216217041016\n",
      "step: 50, train loss: 9.161190032958984, val loss: 9.150064468383789\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 384, 4, 6)\n",
      "step: 0, train loss: 10.438101768493652, val loss: 10.443273544311523\n",
      "step: 50, train loss: 8.566875457763672, val loss: 8.573572158813477\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 384, 4, 10)\n",
      "step: 0, train loss: 10.410470008850098, val loss: 10.408884048461914\n",
      "step: 50, train loss: 8.343777656555176, val loss: 8.330012321472168\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 384, 4, 12)\n",
      "step: 0, train loss: 10.468432426452637, val loss: 10.468283653259277\n",
      "step: 50, train loss: 8.35252571105957, val loss: 8.31064224243164\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 384, 8, 6)\n",
      "step: 0, train loss: 10.424219131469727, val loss: 10.423040390014648\n",
      "step: 50, train loss: 8.731480598449707, val loss: 8.696866989135742\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 384, 8, 10)\n",
      "step: 0, train loss: 10.518915176391602, val loss: 10.51816177368164\n",
      "step: 50, train loss: 8.32227897644043, val loss: 8.297743797302246\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 384, 8, 12)\n",
      "step: 0, train loss: 10.431751251220703, val loss: 10.4340238571167\n",
      "step: 50, train loss: 8.253918647766113, val loss: 8.261775970458984\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 384, 12, 6)\n",
      "step: 0, train loss: 10.439504623413086, val loss: 10.441612243652344\n",
      "step: 50, train loss: 8.783616065979004, val loss: 8.808197021484375\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 384, 12, 10)\n",
      "step: 0, train loss: 10.459744453430176, val loss: 10.458166122436523\n",
      "step: 50, train loss: 8.334280967712402, val loss: 8.376642227172852\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 384, 12, 12)\n",
      "step: 0, train loss: 10.46191120147705, val loss: 10.465372085571289\n",
      "step: 50, train loss: 8.258209228515625, val loss: 8.255640983581543\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 512, 4, 6)\n",
      "step: 0, train loss: 10.470878601074219, val loss: 10.469934463500977\n",
      "step: 50, train loss: 7.981673717498779, val loss: 7.947577476501465\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 512, 4, 10)\n",
      "step: 0, train loss: 10.513803482055664, val loss: 10.510075569152832\n",
      "step: 50, train loss: 7.609844207763672, val loss: 7.627293586730957\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 512, 4, 12)\n",
      "step: 0, train loss: 10.460827827453613, val loss: 10.456872940063477\n",
      "step: 50, train loss: 7.530828952789307, val loss: 7.55720329284668\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 512, 8, 6)\n",
      "step: 0, train loss: 10.502405166625977, val loss: 10.502645492553711\n",
      "step: 50, train loss: 7.8843817710876465, val loss: 7.794425010681152\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 512, 8, 10)\n",
      "step: 0, train loss: 10.4340238571167, val loss: 10.43794059753418\n",
      "step: 50, train loss: 7.572888374328613, val loss: 7.5988311767578125\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 512, 8, 12)\n",
      "step: 0, train loss: 10.531145095825195, val loss: 10.541437149047852\n",
      "step: 50, train loss: 7.682967662811279, val loss: 7.642299175262451\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 512, 12, 6)\n",
      "step: 0, train loss: 10.545714378356934, val loss: 10.542882919311523\n",
      "step: 50, train loss: 7.796968460083008, val loss: 7.774719715118408\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 512, 12, 10)\n",
      "step: 0, train loss: 10.487669944763184, val loss: 10.486847877502441\n",
      "step: 50, train loss: 7.668058395385742, val loss: 7.580526351928711\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 64, 200, 1e-05, 512, 12, 12)\n",
      "step: 0, train loss: 10.475951194763184, val loss: 10.473982810974121\n",
      "step: 50, train loss: 7.544311046600342, val loss: 7.4893927574157715\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 256, 4, 6)\n",
      "step: 0, train loss: 10.421070098876953, val loss: 10.423687934875488\n",
      "step: 50, train loss: 2.9148290157318115, val loss: 2.8544921875\n",
      "step: 100, train loss: 2.6477811336517334, val loss: 2.7020959854125977\n",
      "step: 150, train loss: 2.6893625259399414, val loss: 2.6411919593811035\n",
      "Final loss: 2.6505885124206543\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 256, 4, 10)\n",
      "step: 0, train loss: 10.418712615966797, val loss: 10.418893814086914\n",
      "step: 50, train loss: 2.9440953731536865, val loss: 2.887592077255249\n",
      "step: 100, train loss: 2.7555129528045654, val loss: 2.7520132064819336\n",
      "step: 150, train loss: 2.7011547088623047, val loss: 2.689223527908325\n",
      "Final loss: 2.76387619972229\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 256, 4, 12)\n",
      "step: 0, train loss: 10.441603660583496, val loss: 10.438907623291016\n",
      "step: 50, train loss: 2.812087297439575, val loss: 2.9160730838775635\n",
      "step: 100, train loss: 2.747843027114868, val loss: 2.752509117126465\n",
      "step: 150, train loss: 2.661726474761963, val loss: 2.6549389362335205\n",
      "Final loss: 3.8296377658843994\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 256, 8, 6)\n",
      "step: 0, train loss: 10.444314956665039, val loss: 10.44631576538086\n",
      "step: 50, train loss: 3.053168296813965, val loss: 3.0495855808258057\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 256, 8, 10)\n",
      "step: 0, train loss: 10.414809226989746, val loss: 10.416964530944824\n",
      "step: 50, train loss: 2.968560218811035, val loss: 2.9879627227783203\n",
      "step: 100, train loss: 2.7214319705963135, val loss: 2.6927249431610107\n",
      "step: 150, train loss: 2.650587558746338, val loss: 2.6457409858703613\n",
      "Final loss: 2.597194194793701\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 256, 8, 12)\n",
      "step: 0, train loss: 10.414493560791016, val loss: 10.413385391235352\n",
      "step: 50, train loss: 3.2135682106018066, val loss: 3.282258987426758\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 256, 12, 6)\n",
      "step: 0, train loss: 10.450174331665039, val loss: 10.451398849487305\n",
      "step: 50, train loss: 3.339284896850586, val loss: 3.2490549087524414\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 256, 12, 10)\n",
      "step: 0, train loss: 10.414987564086914, val loss: 10.409987449645996\n",
      "step: 50, train loss: 2.8467981815338135, val loss: 2.861562490463257\n",
      "step: 100, train loss: 2.6806070804595947, val loss: 2.767550468444824\n",
      "step: 150, train loss: 2.7063093185424805, val loss: 2.759540319442749\n",
      "Final loss: 2.6088201999664307\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 256, 12, 12)\n",
      "step: 0, train loss: 10.4617280960083, val loss: 10.464734077453613\n",
      "step: 50, train loss: 2.8402090072631836, val loss: 2.9599664211273193\n",
      "step: 100, train loss: 2.7348456382751465, val loss: 2.6919336318969727\n",
      "step: 150, train loss: 2.7043139934539795, val loss: 2.6546075344085693\n",
      "Final loss: 2.634420871734619\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 384, 4, 6)\n",
      "step: 0, train loss: 10.45556926727295, val loss: 10.4551420211792\n",
      "step: 50, train loss: 3.0108468532562256, val loss: 2.977792739868164\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 384, 4, 10)\n",
      "step: 0, train loss: 10.438109397888184, val loss: 10.44093132019043\n",
      "step: 50, train loss: 3.246116876602173, val loss: 3.2571990489959717\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 384, 4, 12)\n",
      "step: 0, train loss: 10.446512222290039, val loss: 10.447467803955078\n",
      "step: 50, train loss: 3.19925594329834, val loss: 3.217254638671875\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 384, 8, 6)\n",
      "step: 0, train loss: 10.515554428100586, val loss: 10.520358085632324\n",
      "step: 50, train loss: 2.689878225326538, val loss: 2.714520215988159\n",
      "step: 100, train loss: 2.6583662033081055, val loss: 2.6780920028686523\n",
      "step: 150, train loss: 2.6110732555389404, val loss: 2.6171200275421143\n",
      "Final loss: 2.5910348892211914\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 384, 8, 10)\n",
      "step: 0, train loss: 10.491589546203613, val loss: 10.492470741271973\n",
      "step: 50, train loss: 3.4166533946990967, val loss: 3.2852866649627686\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 384, 8, 12)\n",
      "step: 0, train loss: 10.51833724975586, val loss: 10.521373748779297\n",
      "step: 50, train loss: 3.218904495239258, val loss: 3.2074806690216064\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 384, 12, 6)\n",
      "step: 0, train loss: 10.422050476074219, val loss: 10.425698280334473\n",
      "step: 50, train loss: 2.9250149726867676, val loss: 2.9075796604156494\n",
      "step: 100, train loss: 2.7682583332061768, val loss: 2.674697160720825\n",
      "step: 150, train loss: 2.625666856765747, val loss: 2.6826303005218506\n",
      "Final loss: 2.539846420288086\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 384, 12, 10)\n",
      "step: 0, train loss: 10.405462265014648, val loss: 10.407392501831055\n",
      "step: 50, train loss: 3.208834171295166, val loss: 3.4346556663513184\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 384, 12, 12)\n",
      "step: 0, train loss: 10.400036811828613, val loss: 10.407527923583984\n",
      "step: 50, train loss: 3.2372353076934814, val loss: 3.202561378479004\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 512, 4, 6)\n",
      "step: 0, train loss: 10.397151947021484, val loss: 10.399540901184082\n",
      "step: 50, train loss: 3.284827470779419, val loss: 3.2282204627990723\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 512, 4, 10)\n",
      "step: 0, train loss: 10.469674110412598, val loss: 10.468597412109375\n",
      "step: 50, train loss: 3.277271032333374, val loss: 3.2074029445648193\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 512, 4, 12)\n",
      "step: 0, train loss: 10.531586647033691, val loss: 10.528363227844238\n",
      "step: 50, train loss: 3.2630045413970947, val loss: 3.339682102203369\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 512, 8, 6)\n",
      "step: 0, train loss: 10.491235733032227, val loss: 10.495121002197266\n",
      "step: 50, train loss: 3.075385093688965, val loss: 3.041830062866211\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 512, 8, 10)\n",
      "step: 0, train loss: 10.581486701965332, val loss: 10.58098030090332\n",
      "step: 50, train loss: 3.2059860229492188, val loss: 3.234067440032959\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 512, 8, 12)\n",
      "step: 0, train loss: 10.597943305969238, val loss: 10.595008850097656\n",
      "step: 50, train loss: 3.2591588497161865, val loss: 3.2627618312835693\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 512, 12, 6)\n",
      "step: 0, train loss: 10.420893669128418, val loss: 10.426039695739746\n",
      "step: 50, train loss: 3.2334866523742676, val loss: 3.2456116676330566\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 512, 12, 10)\n",
      "step: 0, train loss: 10.495926856994629, val loss: 10.495780944824219\n",
      "step: 50, train loss: 3.2446277141571045, val loss: 3.2203269004821777\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.001, 512, 12, 12)\n",
      "step: 0, train loss: 10.553194999694824, val loss: 10.553518295288086\n",
      "step: 50, train loss: 3.216097354888916, val loss: 3.245450735092163\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 256, 4, 6)\n",
      "step: 0, train loss: 10.4627685546875, val loss: 10.45973014831543\n",
      "step: 50, train loss: 6.965538501739502, val loss: 6.967520713806152\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 256, 4, 10)\n",
      "step: 0, train loss: 10.395389556884766, val loss: 10.392782211303711\n",
      "step: 50, train loss: 6.840933799743652, val loss: 6.828473091125488\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 256, 4, 12)\n",
      "step: 0, train loss: 10.379284858703613, val loss: 10.380030632019043\n",
      "step: 50, train loss: 6.7934746742248535, val loss: 6.784606456756592\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 256, 8, 6)\n",
      "step: 0, train loss: 10.413294792175293, val loss: 10.412955284118652\n",
      "step: 50, train loss: 6.914669990539551, val loss: 6.8916544914245605\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 256, 8, 10)\n",
      "step: 0, train loss: 10.401686668395996, val loss: 10.402112007141113\n",
      "step: 50, train loss: 6.9633097648620605, val loss: 6.94538688659668\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 256, 8, 12)\n",
      "step: 0, train loss: 10.433455467224121, val loss: 10.437080383300781\n",
      "step: 50, train loss: 6.8381218910217285, val loss: 6.833461761474609\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 256, 12, 6)\n",
      "step: 0, train loss: 10.448773384094238, val loss: 10.444012641906738\n",
      "step: 50, train loss: 6.8434672355651855, val loss: 6.851457595825195\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 256, 12, 10)\n",
      "step: 0, train loss: 10.448705673217773, val loss: 10.449785232543945\n",
      "step: 50, train loss: 6.819215297698975, val loss: 6.777569770812988\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 256, 12, 12)\n",
      "step: 0, train loss: 10.432097434997559, val loss: 10.432860374450684\n",
      "step: 50, train loss: 6.916723728179932, val loss: 6.930520057678223\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 384, 4, 6)\n",
      "step: 0, train loss: 10.419720649719238, val loss: 10.420323371887207\n",
      "step: 50, train loss: 5.162791728973389, val loss: 5.157006740570068\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 384, 4, 10)\n",
      "step: 0, train loss: 10.427475929260254, val loss: 10.430228233337402\n",
      "step: 50, train loss: 5.204262733459473, val loss: 5.168829441070557\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 384, 4, 12)\n",
      "step: 0, train loss: 10.362495422363281, val loss: 10.360827445983887\n",
      "step: 50, train loss: 5.225713729858398, val loss: 5.257225513458252\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 384, 8, 6)\n",
      "step: 0, train loss: 10.443670272827148, val loss: 10.440603256225586\n",
      "step: 50, train loss: 5.169278144836426, val loss: 5.184191703796387\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 384, 8, 10)\n",
      "step: 0, train loss: 10.474193572998047, val loss: 10.474272727966309\n",
      "step: 50, train loss: 5.191678047180176, val loss: 5.180395603179932\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 384, 8, 12)\n",
      "step: 0, train loss: 10.433483123779297, val loss: 10.434473037719727\n",
      "step: 50, train loss: 5.248428821563721, val loss: 5.2315239906311035\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 384, 12, 6)\n",
      "step: 0, train loss: 10.444640159606934, val loss: 10.443455696105957\n",
      "step: 50, train loss: 5.228509902954102, val loss: 5.260812759399414\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 384, 12, 10)\n",
      "step: 0, train loss: 10.458715438842773, val loss: 10.454167366027832\n",
      "step: 50, train loss: 5.262969970703125, val loss: 5.2349629402160645\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 384, 12, 12)\n",
      "step: 0, train loss: 10.46945571899414, val loss: 10.467010498046875\n",
      "step: 50, train loss: 5.242725849151611, val loss: 5.191793441772461\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 512, 4, 6)\n",
      "step: 0, train loss: 10.425487518310547, val loss: 10.426957130432129\n",
      "step: 50, train loss: 4.004293918609619, val loss: 4.128686904907227\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 512, 4, 10)\n",
      "step: 0, train loss: 10.368678092956543, val loss: 10.375463485717773\n",
      "step: 50, train loss: 4.049724578857422, val loss: 4.007133960723877\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 512, 4, 12)\n",
      "step: 0, train loss: 10.609387397766113, val loss: 10.607815742492676\n",
      "step: 50, train loss: 4.122713088989258, val loss: 4.0541791915893555\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 512, 8, 6)\n",
      "step: 0, train loss: 10.476253509521484, val loss: 10.476536750793457\n",
      "step: 50, train loss: 4.083682060241699, val loss: 4.043315887451172\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 512, 8, 10)\n",
      "step: 0, train loss: 10.438032150268555, val loss: 10.441035270690918\n",
      "step: 50, train loss: 3.9975638389587402, val loss: 3.990501642227173\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 512, 8, 12)\n",
      "step: 0, train loss: 10.38746452331543, val loss: 10.389570236206055\n",
      "step: 50, train loss: 4.075929164886475, val loss: 3.9973151683807373\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 512, 12, 6)\n",
      "step: 0, train loss: 10.432250022888184, val loss: 10.433669090270996\n",
      "step: 50, train loss: 3.9689557552337646, val loss: 3.977604389190674\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 512, 12, 10)\n",
      "step: 0, train loss: 10.554082870483398, val loss: 10.555874824523926\n",
      "step: 50, train loss: 3.9864346981048584, val loss: 4.018746376037598\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 0.0001, 512, 12, 12)\n",
      "step: 0, train loss: 10.3593111038208, val loss: 10.365711212158203\n",
      "step: 50, train loss: 4.097095489501953, val loss: 4.1224589347839355\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 256, 4, 6)\n",
      "step: 0, train loss: 10.390554428100586, val loss: 10.395044326782227\n",
      "step: 50, train loss: 9.609522819519043, val loss: 9.596898078918457\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 256, 4, 10)\n",
      "step: 0, train loss: 10.423051834106445, val loss: 10.427155494689941\n",
      "step: 50, train loss: 9.212242126464844, val loss: 9.206876754760742\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 256, 4, 12)\n",
      "step: 0, train loss: 10.394676208496094, val loss: 10.397421836853027\n",
      "step: 50, train loss: 9.14436149597168, val loss: 9.154424667358398\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 256, 8, 6)\n",
      "step: 0, train loss: 10.379825592041016, val loss: 10.379952430725098\n",
      "step: 50, train loss: 9.619627952575684, val loss: 9.609699249267578\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 256, 8, 10)\n",
      "step: 0, train loss: 10.406942367553711, val loss: 10.40649700164795\n",
      "step: 50, train loss: 9.28895378112793, val loss: 9.277419090270996\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 256, 8, 12)\n",
      "step: 0, train loss: 10.407000541687012, val loss: 10.408670425415039\n",
      "step: 50, train loss: 9.193599700927734, val loss: 9.173276901245117\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 256, 12, 6)\n",
      "step: 0, train loss: 10.437286376953125, val loss: 10.431373596191406\n",
      "step: 50, train loss: 9.626852989196777, val loss: 9.62572956085205\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 256, 12, 10)\n",
      "step: 0, train loss: 10.446000099182129, val loss: 10.444845199584961\n",
      "step: 50, train loss: 9.341123580932617, val loss: 9.332093238830566\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 256, 12, 12)\n",
      "step: 0, train loss: 10.449682235717773, val loss: 10.45071029663086\n",
      "step: 50, train loss: 9.201576232910156, val loss: 9.231847763061523\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 384, 4, 6)\n",
      "step: 0, train loss: 10.442876815795898, val loss: 10.44084358215332\n",
      "step: 50, train loss: 8.686476707458496, val loss: 8.695088386535645\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 384, 4, 10)\n",
      "step: 0, train loss: 10.469429016113281, val loss: 10.467662811279297\n",
      "step: 50, train loss: 8.378730773925781, val loss: 8.369269371032715\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 384, 4, 12)\n",
      "step: 0, train loss: 10.434971809387207, val loss: 10.436659812927246\n",
      "step: 50, train loss: 8.345823287963867, val loss: 8.327455520629883\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 384, 8, 6)\n",
      "step: 0, train loss: 10.436039924621582, val loss: 10.436670303344727\n",
      "step: 50, train loss: 8.619173049926758, val loss: 8.650447845458984\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 384, 8, 10)\n",
      "step: 0, train loss: 10.426553726196289, val loss: 10.422811508178711\n",
      "step: 50, train loss: 8.388214111328125, val loss: 8.37822437286377\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 384, 8, 12)\n",
      "step: 0, train loss: 10.450139999389648, val loss: 10.448105812072754\n",
      "step: 50, train loss: 8.24899673461914, val loss: 8.242708206176758\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 384, 12, 6)\n",
      "step: 0, train loss: 10.412454605102539, val loss: 10.414510726928711\n",
      "step: 50, train loss: 8.758533477783203, val loss: 8.784153938293457\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 384, 12, 10)\n",
      "step: 0, train loss: 10.396236419677734, val loss: 10.397883415222168\n",
      "step: 50, train loss: 8.319212913513184, val loss: 8.330958366394043\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 384, 12, 12)\n",
      "step: 0, train loss: 10.467534065246582, val loss: 10.469609260559082\n",
      "step: 50, train loss: 8.141691207885742, val loss: 8.131431579589844\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 512, 4, 6)\n",
      "step: 0, train loss: 10.519312858581543, val loss: 10.521100997924805\n",
      "step: 50, train loss: 7.704500675201416, val loss: 7.712373733520508\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 512, 4, 10)\n",
      "step: 0, train loss: 10.54117202758789, val loss: 10.544483184814453\n",
      "step: 50, train loss: 7.455738544464111, val loss: 7.440270900726318\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 512, 4, 12)\n",
      "step: 0, train loss: 10.476995468139648, val loss: 10.47415542602539\n",
      "step: 50, train loss: 7.512291431427002, val loss: 7.602532863616943\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 512, 8, 6)\n",
      "step: 0, train loss: 10.511160850524902, val loss: 10.510093688964844\n",
      "step: 50, train loss: 7.681136608123779, val loss: 7.677828311920166\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 512, 8, 10)\n",
      "step: 0, train loss: 10.49548053741455, val loss: 10.492958068847656\n",
      "step: 50, train loss: 7.641139984130859, val loss: 7.671779632568359\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 512, 8, 12)\n",
      "step: 0, train loss: 10.3899564743042, val loss: 10.389145851135254\n",
      "step: 50, train loss: 7.509047031402588, val loss: 7.4807891845703125\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 512, 12, 6)\n",
      "step: 0, train loss: 10.549101829528809, val loss: 10.545437812805176\n",
      "step: 50, train loss: 7.851443290710449, val loss: 7.8666911125183105\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 512, 12, 10)\n",
      "step: 0, train loss: 10.46853256225586, val loss: 10.475166320800781\n",
      "step: 50, train loss: 7.415392875671387, val loss: 7.413622856140137\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 128, 200, 1e-05, 512, 12, 12)\n",
      "step: 0, train loss: 10.507235527038574, val loss: 10.50459098815918\n",
      "step: 50, train loss: 7.551382064819336, val loss: 7.544577121734619\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 256, 4, 6)\n",
      "step: 0, train loss: 10.409454345703125, val loss: 10.409794807434082\n",
      "step: 50, train loss: 2.840108633041382, val loss: 2.9867546558380127\n",
      "step: 100, train loss: 2.6621930599212646, val loss: 2.678152084350586\n",
      "step: 150, train loss: 2.6391477584838867, val loss: 2.647326707839966\n",
      "Final loss: 2.625617265701294\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 256, 4, 10)\n",
      "step: 0, train loss: 10.397974967956543, val loss: 10.39710807800293\n",
      "step: 50, train loss: 3.215162754058838, val loss: 3.2043588161468506\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 256, 4, 12)\n",
      "step: 0, train loss: 10.378271102905273, val loss: 10.379284858703613\n",
      "step: 50, train loss: 2.7745161056518555, val loss: 2.7355728149414062\n",
      "step: 100, train loss: 2.7533769607543945, val loss: 2.6700286865234375\n",
      "step: 150, train loss: 2.6463358402252197, val loss: 2.6549551486968994\n",
      "Final loss: 2.5368804931640625\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 256, 8, 6)\n",
      "step: 0, train loss: 10.450401306152344, val loss: 10.453782081604004\n",
      "step: 50, train loss: 2.790099859237671, val loss: 2.733229160308838\n",
      "step: 100, train loss: 2.685986280441284, val loss: 2.6614365577697754\n",
      "step: 150, train loss: 2.6018612384796143, val loss: 2.6206841468811035\n",
      "Final loss: 2.558439016342163\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 256, 8, 10)\n",
      "step: 0, train loss: 10.383366584777832, val loss: 10.381630897521973\n",
      "step: 50, train loss: 2.778101921081543, val loss: 2.7773799896240234\n",
      "step: 100, train loss: 2.674025535583496, val loss: 2.6732089519500732\n",
      "step: 150, train loss: 2.615842342376709, val loss: 2.6744165420532227\n",
      "Final loss: 2.6284244060516357\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 256, 8, 12)\n",
      "step: 0, train loss: 10.40099811553955, val loss: 10.400976181030273\n",
      "step: 50, train loss: 2.7800002098083496, val loss: 2.7599897384643555\n",
      "step: 100, train loss: 2.731024742126465, val loss: 2.6565163135528564\n",
      "step: 150, train loss: 2.6896026134490967, val loss: 2.6033565998077393\n",
      "Final loss: 2.569565773010254\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 256, 12, 6)\n",
      "step: 0, train loss: 10.415389060974121, val loss: 10.413827896118164\n",
      "step: 50, train loss: 3.046954870223999, val loss: 2.9372010231018066\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 256, 12, 10)\n",
      "step: 0, train loss: 10.396775245666504, val loss: 10.397730827331543\n",
      "step: 50, train loss: 2.751812219619751, val loss: 2.7350456714630127\n",
      "step: 100, train loss: 2.6659491062164307, val loss: 2.6409738063812256\n",
      "step: 150, train loss: 2.6660001277923584, val loss: 2.686877727508545\n",
      "Final loss: 2.679710865020752\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 256, 12, 12)\n",
      "step: 0, train loss: 10.45751953125, val loss: 10.456108093261719\n",
      "step: 50, train loss: 2.9793126583099365, val loss: 2.9434728622436523\n",
      "step: 100, train loss: 2.6966662406921387, val loss: 2.6949679851531982\n",
      "step: 150, train loss: 2.704263210296631, val loss: 2.6903951168060303\n",
      "Final loss: 2.544398546218872\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 384, 4, 6)\n",
      "step: 0, train loss: 10.47619342803955, val loss: 10.479076385498047\n",
      "step: 50, train loss: 2.703352451324463, val loss: 2.9246320724487305\n",
      "step: 100, train loss: 2.73197865486145, val loss: 2.699005126953125\n",
      "step: 150, train loss: 2.5934524536132812, val loss: 2.673866033554077\n",
      "Final loss: 2.7012057304382324\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 384, 4, 10)\n",
      "step: 0, train loss: 10.442913055419922, val loss: 10.445944786071777\n",
      "step: 50, train loss: 3.230461359024048, val loss: 3.2327065467834473\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 384, 4, 12)\n",
      "step: 0, train loss: 10.45459270477295, val loss: 10.450383186340332\n",
      "step: 50, train loss: 3.220188617706299, val loss: 3.2220284938812256\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 384, 8, 6)\n",
      "step: 0, train loss: 10.493032455444336, val loss: 10.491859436035156\n",
      "step: 50, train loss: 2.752455949783325, val loss: 2.671645402908325\n",
      "step: 100, train loss: 2.644510507583618, val loss: 2.635113477706909\n",
      "step: 150, train loss: 2.6499972343444824, val loss: 2.6434946060180664\n",
      "Final loss: 2.5756642818450928\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 384, 8, 10)\n",
      "step: 0, train loss: 10.352294921875, val loss: 10.35205364227295\n",
      "step: 50, train loss: 3.2482945919036865, val loss: 3.2007856369018555\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 384, 8, 12)\n",
      "step: 0, train loss: 10.514543533325195, val loss: 10.51489543914795\n",
      "step: 50, train loss: 3.2142577171325684, val loss: 3.205670118331909\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 384, 12, 6)\n",
      "step: 0, train loss: 10.409272193908691, val loss: 10.40951919555664\n",
      "step: 50, train loss: 2.706615686416626, val loss: 2.7495453357696533\n",
      "step: 100, train loss: 2.658378839492798, val loss: 2.6876327991485596\n",
      "step: 150, train loss: 2.6338186264038086, val loss: 2.6896560192108154\n",
      "Final loss: 2.6486854553222656\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 384, 12, 10)\n",
      "step: 0, train loss: 10.484479904174805, val loss: 10.481806755065918\n",
      "step: 50, train loss: 3.1868560314178467, val loss: 3.2034947872161865\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 384, 12, 12)\n",
      "step: 0, train loss: 10.413969993591309, val loss: 10.417296409606934\n",
      "step: 50, train loss: 3.2207934856414795, val loss: 3.28940486907959\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 512, 4, 6)\n",
      "step: 0, train loss: 10.512943267822266, val loss: 10.510797500610352\n",
      "step: 50, train loss: 2.7428340911865234, val loss: 2.749347448348999\n",
      "step: 100, train loss: 2.6903533935546875, val loss: 2.8055503368377686\n",
      "step: 150, train loss: 2.674245834350586, val loss: 2.8908536434173584\n",
      "Final loss: 2.7706170082092285\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 512, 4, 10)\n",
      "step: 0, train loss: 10.449982643127441, val loss: 10.44814682006836\n",
      "step: 50, train loss: 3.2804465293884277, val loss: 3.1941442489624023\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 512, 4, 12)\n",
      "step: 0, train loss: 10.558263778686523, val loss: 10.555956840515137\n",
      "step: 50, train loss: 3.266113042831421, val loss: 3.2657904624938965\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 512, 8, 6)\n",
      "step: 0, train loss: 10.459178924560547, val loss: 10.461705207824707\n",
      "step: 50, train loss: 3.1363635063171387, val loss: 3.1694257259368896\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 512, 8, 10)\n",
      "step: 0, train loss: 10.58159065246582, val loss: 10.575966835021973\n",
      "step: 50, train loss: 3.1907827854156494, val loss: 3.176924705505371\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 512, 8, 12)\n",
      "step: 0, train loss: 10.448755264282227, val loss: 10.44705581665039\n",
      "step: 50, train loss: 3.2594661712646484, val loss: 3.2082204818725586\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 512, 12, 6)\n",
      "step: 0, train loss: 10.518590927124023, val loss: 10.514969825744629\n",
      "step: 50, train loss: 3.2936151027679443, val loss: 3.214047431945801\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 512, 12, 10)\n",
      "step: 0, train loss: 10.482068061828613, val loss: 10.483675956726074\n",
      "step: 50, train loss: 3.247946262359619, val loss: 3.3248562812805176\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.001, 512, 12, 12)\n",
      "step: 0, train loss: 10.492121696472168, val loss: 10.487199783325195\n",
      "step: 50, train loss: 3.241250514984131, val loss: 3.1993775367736816\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 256, 4, 6)\n",
      "step: 0, train loss: 10.428974151611328, val loss: 10.428986549377441\n",
      "step: 50, train loss: 6.850778579711914, val loss: 6.855710506439209\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 256, 4, 10)\n",
      "step: 0, train loss: 10.467970848083496, val loss: 10.464982032775879\n",
      "step: 50, train loss: 6.880658149719238, val loss: 6.958547592163086\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 256, 4, 12)\n",
      "step: 0, train loss: 10.413631439208984, val loss: 10.415227890014648\n",
      "step: 50, train loss: 6.7931904792785645, val loss: 6.790119647979736\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 256, 8, 6)\n",
      "step: 0, train loss: 10.435090065002441, val loss: 10.43536376953125\n",
      "step: 50, train loss: 6.8201680183410645, val loss: 6.822551250457764\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 256, 8, 10)\n",
      "step: 0, train loss: 10.418423652648926, val loss: 10.418719291687012\n",
      "step: 50, train loss: 6.892722129821777, val loss: 6.895503997802734\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 256, 8, 12)\n",
      "step: 0, train loss: 10.474015235900879, val loss: 10.474625587463379\n",
      "step: 50, train loss: 6.819753646850586, val loss: 6.954600811004639\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 256, 12, 6)\n",
      "step: 0, train loss: 10.397831916809082, val loss: 10.3984956741333\n",
      "step: 50, train loss: 6.849857807159424, val loss: 6.834715366363525\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 256, 12, 10)\n",
      "step: 0, train loss: 10.364651679992676, val loss: 10.368317604064941\n",
      "step: 50, train loss: 6.7236151695251465, val loss: 6.713152885437012\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 256, 12, 12)\n",
      "step: 0, train loss: 10.418612480163574, val loss: 10.416118621826172\n",
      "step: 50, train loss: 6.879634380340576, val loss: 6.853267192840576\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 384, 4, 6)\n",
      "step: 0, train loss: 10.476731300354004, val loss: 10.478850364685059\n",
      "step: 50, train loss: 5.158954620361328, val loss: 5.202549934387207\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 384, 4, 10)\n",
      "step: 0, train loss: 10.44815444946289, val loss: 10.448655128479004\n",
      "step: 50, train loss: 5.152413845062256, val loss: 5.181766986846924\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 384, 4, 12)\n",
      "step: 0, train loss: 10.472015380859375, val loss: 10.46884536743164\n",
      "step: 50, train loss: 5.284664154052734, val loss: 5.268450736999512\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 384, 8, 6)\n",
      "step: 0, train loss: 10.515447616577148, val loss: 10.513562202453613\n",
      "step: 50, train loss: 5.225536346435547, val loss: 5.212812423706055\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 384, 8, 10)\n",
      "step: 0, train loss: 10.458719253540039, val loss: 10.454263687133789\n",
      "step: 50, train loss: 5.12955379486084, val loss: 5.118378162384033\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 384, 8, 12)\n",
      "step: 0, train loss: 10.52046012878418, val loss: 10.519737243652344\n",
      "step: 50, train loss: 5.172592639923096, val loss: 5.22730827331543\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 384, 12, 6)\n",
      "step: 0, train loss: 10.404463768005371, val loss: 10.403111457824707\n",
      "step: 50, train loss: 5.253244400024414, val loss: 5.251174449920654\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 384, 12, 10)\n",
      "step: 0, train loss: 10.34714126586914, val loss: 10.34737777709961\n",
      "step: 50, train loss: 5.195296764373779, val loss: 5.170599937438965\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 384, 12, 12)\n",
      "step: 0, train loss: 10.361225128173828, val loss: 10.358237266540527\n",
      "step: 50, train loss: 5.143928050994873, val loss: 5.1282548904418945\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 512, 4, 6)\n",
      "step: 0, train loss: 10.439657211303711, val loss: 10.445873260498047\n",
      "step: 50, train loss: 3.9171524047851562, val loss: 3.957207679748535\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 512, 4, 10)\n",
      "step: 0, train loss: 10.466853141784668, val loss: 10.47050666809082\n",
      "step: 50, train loss: 4.000360012054443, val loss: 4.0782365798950195\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 512, 4, 12)\n",
      "step: 0, train loss: 10.51333236694336, val loss: 10.511087417602539\n",
      "step: 50, train loss: 3.982720375061035, val loss: 4.023626327514648\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 512, 8, 6)\n",
      "step: 0, train loss: 10.444716453552246, val loss: 10.443979263305664\n",
      "step: 50, train loss: 3.9730918407440186, val loss: 3.948978900909424\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 512, 8, 10)\n",
      "step: 0, train loss: 10.465476036071777, val loss: 10.46501350402832\n",
      "step: 50, train loss: 4.0171403884887695, val loss: 4.157671928405762\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 512, 8, 12)\n",
      "step: 0, train loss: 10.384749412536621, val loss: 10.385082244873047\n",
      "step: 50, train loss: 3.971402645111084, val loss: 3.949570894241333\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 512, 12, 6)\n",
      "step: 0, train loss: 10.438249588012695, val loss: 10.43551254272461\n",
      "step: 50, train loss: 4.01639986038208, val loss: 3.9887781143188477\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 512, 12, 10)\n",
      "step: 0, train loss: 10.557616233825684, val loss: 10.556168556213379\n",
      "step: 50, train loss: 3.9504506587982178, val loss: 3.954312801361084\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 0.0001, 512, 12, 12)\n",
      "step: 0, train loss: 10.454157829284668, val loss: 10.45307731628418\n",
      "step: 50, train loss: 4.05590295791626, val loss: 4.0154500007629395\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 256, 4, 6)\n",
      "step: 0, train loss: 10.414532661437988, val loss: 10.418501853942871\n",
      "step: 50, train loss: 9.61967945098877, val loss: 9.605989456176758\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 256, 4, 10)\n",
      "step: 0, train loss: 10.405316352844238, val loss: 10.404605865478516\n",
      "step: 50, train loss: 9.273956298828125, val loss: 9.292247772216797\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 256, 4, 12)\n",
      "step: 0, train loss: 10.428752899169922, val loss: 10.428742408752441\n",
      "step: 50, train loss: 9.132401466369629, val loss: 9.112879753112793\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 256, 8, 6)\n",
      "step: 0, train loss: 10.447392463684082, val loss: 10.445883750915527\n",
      "step: 50, train loss: 9.670401573181152, val loss: 9.662544250488281\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 256, 8, 10)\n",
      "step: 0, train loss: 10.422987937927246, val loss: 10.419690132141113\n",
      "step: 50, train loss: 9.271688461303711, val loss: 9.246231079101562\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 256, 8, 12)\n",
      "step: 0, train loss: 10.381797790527344, val loss: 10.384268760681152\n",
      "step: 50, train loss: 9.150773048400879, val loss: 9.143697738647461\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 256, 12, 6)\n",
      "step: 0, train loss: 10.361379623413086, val loss: 10.35882568359375\n",
      "step: 50, train loss: 9.526302337646484, val loss: 9.535828590393066\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 256, 12, 10)\n",
      "step: 0, train loss: 10.437065124511719, val loss: 10.437444686889648\n",
      "step: 50, train loss: 9.333229064941406, val loss: 9.336115837097168\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 256, 12, 12)\n",
      "step: 0, train loss: 10.439874649047852, val loss: 10.439861297607422\n",
      "step: 50, train loss: 9.159271240234375, val loss: 9.167866706848145\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 384, 4, 6)\n",
      "step: 0, train loss: 10.41055679321289, val loss: 10.410594940185547\n",
      "step: 50, train loss: 8.639351844787598, val loss: 8.652538299560547\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 384, 4, 10)\n",
      "step: 0, train loss: 10.42534065246582, val loss: 10.42721939086914\n",
      "step: 50, train loss: 8.294136047363281, val loss: 8.284219741821289\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 384, 4, 12)\n",
      "step: 0, train loss: 10.569635391235352, val loss: 10.56688117980957\n",
      "step: 50, train loss: 8.264664649963379, val loss: 8.202469825744629\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 384, 8, 6)\n",
      "step: 0, train loss: 10.455413818359375, val loss: 10.455120086669922\n",
      "step: 50, train loss: 8.727287292480469, val loss: 8.71892261505127\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 384, 8, 10)\n",
      "step: 0, train loss: 10.47294807434082, val loss: 10.470763206481934\n",
      "step: 50, train loss: 8.339787483215332, val loss: 8.370214462280273\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 384, 8, 12)\n",
      "step: 0, train loss: 10.501492500305176, val loss: 10.504446029663086\n",
      "step: 50, train loss: 8.26465129852295, val loss: 8.255465507507324\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 384, 12, 6)\n",
      "step: 0, train loss: 10.459216117858887, val loss: 10.459192276000977\n",
      "step: 50, train loss: 8.794373512268066, val loss: 8.775866508483887\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 384, 12, 10)\n",
      "step: 0, train loss: 10.469280242919922, val loss: 10.468231201171875\n",
      "step: 50, train loss: 8.426799774169922, val loss: 8.391423225402832\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 384, 12, 12)\n",
      "step: 0, train loss: 10.516566276550293, val loss: 10.518272399902344\n",
      "step: 50, train loss: 8.321599960327148, val loss: 8.316086769104004\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 512, 4, 6)\n",
      "step: 0, train loss: 10.50805377960205, val loss: 10.504846572875977\n",
      "step: 50, train loss: 7.816553115844727, val loss: 7.7959818840026855\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 512, 4, 10)\n",
      "step: 0, train loss: 10.401988983154297, val loss: 10.401196479797363\n",
      "step: 50, train loss: 7.488915920257568, val loss: 7.491943836212158\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 512, 4, 12)\n",
      "step: 0, train loss: 10.461555480957031, val loss: 10.465646743774414\n",
      "step: 50, train loss: 7.6099677085876465, val loss: 7.622501373291016\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 512, 8, 6)\n",
      "step: 0, train loss: 10.52760124206543, val loss: 10.532386779785156\n",
      "step: 50, train loss: 7.765795707702637, val loss: 7.8274970054626465\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 512, 8, 10)\n",
      "step: 0, train loss: 10.538304328918457, val loss: 10.5349702835083\n",
      "step: 50, train loss: 7.544022083282471, val loss: 7.616441249847412\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 512, 8, 12)\n",
      "step: 0, train loss: 10.513847351074219, val loss: 10.511013984680176\n",
      "step: 50, train loss: 7.514631748199463, val loss: 7.527295112609863\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 512, 12, 6)\n",
      "step: 0, train loss: 10.59267807006836, val loss: 10.593853950500488\n",
      "step: 50, train loss: 7.793138027191162, val loss: 7.7662835121154785\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 512, 12, 10)\n",
      "step: 0, train loss: 10.452832221984863, val loss: 10.455362319946289\n",
      "step: 50, train loss: 7.6556596755981445, val loss: 7.698462009429932\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (16, 256, 200, 1e-05, 512, 12, 12)\n",
      "step: 0, train loss: 10.445662498474121, val loss: 10.442663192749023\n",
      "step: 50, train loss: 7.386327743530273, val loss: 7.399708271026611\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 256, 4, 6)\n",
      "step: 0, train loss: 10.427752494812012, val loss: 10.42813491821289\n",
      "step: 50, train loss: 2.825805425643921, val loss: 2.7652173042297363\n",
      "step: 100, train loss: 2.6626877784729004, val loss: 2.6371264457702637\n",
      "step: 150, train loss: 2.698776960372925, val loss: 2.6108710765838623\n",
      "Final loss: 2.467355966567993\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 256, 4, 10)\n",
      "step: 0, train loss: 10.38967514038086, val loss: 10.387454986572266\n",
      "step: 50, train loss: 3.0305614471435547, val loss: 2.966923713684082\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 256, 4, 12)\n",
      "step: 0, train loss: 10.398346900939941, val loss: 10.396196365356445\n",
      "step: 50, train loss: 3.2166812419891357, val loss: 3.1999285221099854\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 256, 8, 6)\n",
      "step: 0, train loss: 10.445347785949707, val loss: 10.449166297912598\n",
      "step: 50, train loss: 3.0774919986724854, val loss: 3.1086695194244385\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 256, 8, 10)\n",
      "step: 0, train loss: 10.460546493530273, val loss: 10.463518142700195\n",
      "step: 50, train loss: 2.801079750061035, val loss: 2.8079047203063965\n",
      "step: 100, train loss: 2.6841237545013428, val loss: 2.707123041152954\n",
      "step: 150, train loss: 2.70070219039917, val loss: 2.6181859970092773\n",
      "Final loss: 2.520181894302368\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 256, 8, 12)\n",
      "step: 0, train loss: 10.440546035766602, val loss: 10.440099716186523\n",
      "step: 50, train loss: 3.428116798400879, val loss: 3.165287971496582\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 256, 12, 6)\n",
      "step: 0, train loss: 10.42446517944336, val loss: 10.42862319946289\n",
      "step: 50, train loss: 3.2284111976623535, val loss: 3.2887816429138184\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 256, 12, 10)\n",
      "step: 0, train loss: 10.467901229858398, val loss: 10.468265533447266\n",
      "step: 50, train loss: 2.749739408493042, val loss: 2.7553908824920654\n",
      "step: 100, train loss: 2.6790037155151367, val loss: 2.689565420150757\n",
      "step: 150, train loss: 2.5766501426696777, val loss: 2.6080586910247803\n",
      "Final loss: 2.630585193634033\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 256, 12, 12)\n",
      "step: 0, train loss: 10.470362663269043, val loss: 10.471335411071777\n",
      "step: 50, train loss: 3.1577672958374023, val loss: 3.1669201850891113\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 384, 4, 6)\n",
      "step: 0, train loss: 10.390471458435059, val loss: 10.395983695983887\n",
      "step: 50, train loss: 2.7944812774658203, val loss: 2.741243362426758\n",
      "step: 100, train loss: 2.689093828201294, val loss: 2.6475515365600586\n",
      "step: 150, train loss: 2.577511787414551, val loss: 2.630833387374878\n",
      "Final loss: 2.5086817741394043\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 384, 4, 10)\n",
      "step: 0, train loss: 10.476360321044922, val loss: 10.473282814025879\n",
      "step: 50, train loss: 3.2053472995758057, val loss: 3.1791858673095703\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 384, 4, 12)\n",
      "step: 0, train loss: 10.441930770874023, val loss: 10.44343376159668\n",
      "step: 50, train loss: 3.261948823928833, val loss: 3.2047410011291504\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 384, 8, 6)\n",
      "step: 0, train loss: 10.438380241394043, val loss: 10.441737174987793\n",
      "step: 50, train loss: 2.723829984664917, val loss: 2.7510745525360107\n",
      "step: 100, train loss: 2.669166326522827, val loss: 2.665738105773926\n",
      "step: 150, train loss: 2.678877592086792, val loss: 2.65255069732666\n",
      "Final loss: 2.7838613986968994\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 384, 8, 10)\n",
      "step: 0, train loss: 10.386519432067871, val loss: 10.385586738586426\n",
      "step: 50, train loss: 3.248124599456787, val loss: 3.2232532501220703\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 384, 8, 12)\n",
      "step: 0, train loss: 10.4552001953125, val loss: 10.451391220092773\n",
      "step: 50, train loss: 3.235577344894409, val loss: 3.2497315406799316\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 384, 12, 6)\n",
      "step: 0, train loss: 10.518357276916504, val loss: 10.51811408996582\n",
      "step: 50, train loss: 2.696545362472534, val loss: 2.682788372039795\n",
      "step: 100, train loss: 2.6312475204467773, val loss: 2.6552176475524902\n",
      "step: 150, train loss: 2.713014841079712, val loss: 2.631523370742798\n",
      "Final loss: 2.542449712753296\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 384, 12, 10)\n",
      "step: 0, train loss: 10.436171531677246, val loss: 10.434300422668457\n",
      "step: 50, train loss: 3.244534492492676, val loss: 3.2946431636810303\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 384, 12, 12)\n",
      "step: 0, train loss: 10.434989929199219, val loss: 10.435009956359863\n",
      "step: 50, train loss: 3.22426700592041, val loss: 3.2289316654205322\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 512, 4, 6)\n",
      "step: 0, train loss: 10.467669486999512, val loss: 10.465750694274902\n",
      "step: 50, train loss: 3.3041861057281494, val loss: 3.227961540222168\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 512, 4, 10)\n",
      "step: 0, train loss: 10.50579833984375, val loss: 10.5049409866333\n",
      "step: 50, train loss: 3.252346992492676, val loss: 3.234264850616455\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 512, 4, 12)\n",
      "step: 0, train loss: 10.45632553100586, val loss: 10.451109886169434\n",
      "step: 50, train loss: 3.2806355953216553, val loss: 3.224824905395508\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 512, 8, 6)\n",
      "step: 0, train loss: 10.382716178894043, val loss: 10.3829927444458\n",
      "step: 50, train loss: 3.0401718616485596, val loss: 2.9566192626953125\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 512, 8, 10)\n",
      "step: 0, train loss: 10.497310638427734, val loss: 10.4952392578125\n",
      "step: 50, train loss: 3.2610156536102295, val loss: 3.6029064655303955\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 512, 8, 12)\n",
      "step: 0, train loss: 10.547385215759277, val loss: 10.546281814575195\n",
      "step: 50, train loss: 3.4511427879333496, val loss: 3.2286322116851807\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 512, 12, 6)\n",
      "step: 0, train loss: 10.411986351013184, val loss: 10.410826683044434\n",
      "step: 50, train loss: 3.234773635864258, val loss: 3.1962084770202637\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 512, 12, 10)\n",
      "step: 0, train loss: 10.530235290527344, val loss: 10.529703140258789\n",
      "step: 50, train loss: 3.2390918731689453, val loss: 3.267875909805298\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.001, 512, 12, 12)\n",
      "step: 0, train loss: 10.443245887756348, val loss: 10.441658973693848\n",
      "step: 50, train loss: 3.248061180114746, val loss: 3.2241933345794678\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 256, 4, 6)\n",
      "step: 0, train loss: 10.424690246582031, val loss: 10.426012992858887\n",
      "step: 50, train loss: 6.917209625244141, val loss: 6.920274257659912\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 256, 4, 10)\n",
      "step: 0, train loss: 10.413520812988281, val loss: 10.412707328796387\n",
      "step: 50, train loss: 6.8759379386901855, val loss: 6.8805766105651855\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 256, 4, 12)\n",
      "step: 0, train loss: 10.480838775634766, val loss: 10.485838890075684\n",
      "step: 50, train loss: 6.8649115562438965, val loss: 6.883438587188721\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 256, 8, 6)\n",
      "step: 0, train loss: 10.415539741516113, val loss: 10.416499137878418\n",
      "step: 50, train loss: 6.703083038330078, val loss: 6.7069993019104\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 256, 8, 10)\n",
      "step: 0, train loss: 10.45923137664795, val loss: 10.459450721740723\n",
      "step: 50, train loss: 6.831323623657227, val loss: 6.855838775634766\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 256, 8, 12)\n",
      "step: 0, train loss: 10.406886100769043, val loss: 10.403115272521973\n",
      "step: 50, train loss: 6.831723213195801, val loss: 6.819709300994873\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 256, 12, 6)\n",
      "step: 0, train loss: 10.40608024597168, val loss: 10.404322624206543\n",
      "step: 50, train loss: 6.909897327423096, val loss: 6.9207024574279785\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 256, 12, 10)\n",
      "step: 0, train loss: 10.425087928771973, val loss: 10.42881965637207\n",
      "step: 50, train loss: 6.8719401359558105, val loss: 6.904941558837891\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 256, 12, 12)\n",
      "step: 0, train loss: 10.437732696533203, val loss: 10.434958457946777\n",
      "step: 50, train loss: 6.917623996734619, val loss: 6.910534858703613\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 384, 4, 6)\n",
      "step: 0, train loss: 10.506946563720703, val loss: 10.5056734085083\n",
      "step: 50, train loss: 5.251605033874512, val loss: 5.211553573608398\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 384, 4, 10)\n",
      "step: 0, train loss: 10.464144706726074, val loss: 10.464072227478027\n",
      "step: 50, train loss: 5.2213311195373535, val loss: 5.138118267059326\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 384, 4, 12)\n",
      "step: 0, train loss: 10.431110382080078, val loss: 10.426445007324219\n",
      "step: 50, train loss: 5.173858642578125, val loss: 5.218339920043945\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 384, 8, 6)\n",
      "step: 0, train loss: 10.455352783203125, val loss: 10.454632759094238\n",
      "step: 50, train loss: 5.2841901779174805, val loss: 5.265240669250488\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 384, 8, 10)\n",
      "step: 0, train loss: 10.4169921875, val loss: 10.42424201965332\n",
      "step: 50, train loss: 5.158412933349609, val loss: 5.200801372528076\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 384, 8, 12)\n",
      "step: 0, train loss: 10.55456256866455, val loss: 10.549875259399414\n",
      "step: 50, train loss: 5.192868232727051, val loss: 5.2135210037231445\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 384, 12, 6)\n",
      "step: 0, train loss: 10.454257011413574, val loss: 10.453988075256348\n",
      "step: 50, train loss: 5.265222549438477, val loss: 5.250653266906738\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 384, 12, 10)\n",
      "step: 0, train loss: 10.464366912841797, val loss: 10.461979866027832\n",
      "step: 50, train loss: 5.274245738983154, val loss: 5.2279205322265625\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 384, 12, 12)\n",
      "step: 0, train loss: 10.424529075622559, val loss: 10.429055213928223\n",
      "step: 50, train loss: 5.146157741546631, val loss: 5.098200798034668\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 512, 4, 6)\n",
      "step: 0, train loss: 10.579833984375, val loss: 10.57946491241455\n",
      "step: 50, train loss: 4.0197062492370605, val loss: 4.044513702392578\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 512, 4, 10)\n",
      "step: 0, train loss: 10.437844276428223, val loss: 10.434697151184082\n",
      "step: 50, train loss: 4.0700554847717285, val loss: 4.080047130584717\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 512, 4, 12)\n",
      "step: 0, train loss: 10.4866943359375, val loss: 10.487299919128418\n",
      "step: 50, train loss: 4.0215044021606445, val loss: 4.0148844718933105\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 512, 8, 6)\n",
      "step: 0, train loss: 10.509622573852539, val loss: 10.507165908813477\n",
      "step: 50, train loss: 4.047730922698975, val loss: 4.0911478996276855\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 512, 8, 10)\n",
      "step: 0, train loss: 10.547207832336426, val loss: 10.542641639709473\n",
      "step: 50, train loss: 3.9430882930755615, val loss: 3.90211820602417\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 512, 8, 12)\n",
      "step: 0, train loss: 10.455893516540527, val loss: 10.448101997375488\n",
      "step: 50, train loss: 4.121917247772217, val loss: 4.124736785888672\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 512, 12, 6)\n",
      "step: 0, train loss: 10.49641227722168, val loss: 10.494424819946289\n",
      "step: 50, train loss: 3.9116852283477783, val loss: 3.9545485973358154\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 512, 12, 10)\n",
      "step: 0, train loss: 10.442652702331543, val loss: 10.443938255310059\n",
      "step: 50, train loss: 4.032037258148193, val loss: 4.047951698303223\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 0.0001, 512, 12, 12)\n",
      "step: 0, train loss: 10.584454536437988, val loss: 10.581352233886719\n",
      "step: 50, train loss: 4.027853012084961, val loss: 4.0178632736206055\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 256, 4, 6)\n",
      "step: 0, train loss: 10.411701202392578, val loss: 10.408742904663086\n",
      "step: 50, train loss: 9.631662368774414, val loss: 9.62643051147461\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 256, 4, 10)\n",
      "step: 0, train loss: 10.417841911315918, val loss: 10.417567253112793\n",
      "step: 50, train loss: 9.32686710357666, val loss: 9.31557846069336\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 256, 4, 12)\n",
      "step: 0, train loss: 10.460346221923828, val loss: 10.465473175048828\n",
      "step: 50, train loss: 9.233194351196289, val loss: 9.23917007446289\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 256, 8, 6)\n",
      "step: 0, train loss: 10.440997123718262, val loss: 10.442765235900879\n",
      "step: 50, train loss: 9.682417869567871, val loss: 9.666319847106934\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 256, 8, 10)\n",
      "step: 0, train loss: 10.463349342346191, val loss: 10.463830947875977\n",
      "step: 50, train loss: 9.237113952636719, val loss: 9.234548568725586\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 256, 8, 12)\n",
      "step: 0, train loss: 10.450569152832031, val loss: 10.452183723449707\n",
      "step: 50, train loss: 9.216635704040527, val loss: 9.234160423278809\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 256, 12, 6)\n",
      "step: 0, train loss: 10.436739921569824, val loss: 10.440498352050781\n",
      "step: 50, train loss: 9.709263801574707, val loss: 9.685571670532227\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 256, 12, 10)\n",
      "step: 0, train loss: 10.51977825164795, val loss: 10.523416519165039\n",
      "step: 50, train loss: 9.379865646362305, val loss: 9.378205299377441\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 256, 12, 12)\n",
      "step: 0, train loss: 10.424599647521973, val loss: 10.426187515258789\n",
      "step: 50, train loss: 9.135586738586426, val loss: 9.142784118652344\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 384, 4, 6)\n",
      "step: 0, train loss: 10.447797775268555, val loss: 10.450840950012207\n",
      "step: 50, train loss: 8.674857139587402, val loss: 8.664652824401855\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 384, 4, 10)\n",
      "step: 0, train loss: 10.48620319366455, val loss: 10.48483657836914\n",
      "step: 50, train loss: 8.493043899536133, val loss: 8.478534698486328\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 384, 4, 12)\n",
      "step: 0, train loss: 10.503711700439453, val loss: 10.504427909851074\n",
      "step: 50, train loss: 8.398419380187988, val loss: 8.387770652770996\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 384, 8, 6)\n",
      "step: 0, train loss: 10.443150520324707, val loss: 10.443700790405273\n",
      "step: 50, train loss: 8.688281059265137, val loss: 8.735520362854004\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 384, 8, 10)\n",
      "step: 0, train loss: 10.417607307434082, val loss: 10.416857719421387\n",
      "step: 50, train loss: 8.465932846069336, val loss: 8.487144470214844\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 384, 8, 12)\n",
      "step: 0, train loss: 10.495892524719238, val loss: 10.496729850769043\n",
      "step: 50, train loss: 8.307140350341797, val loss: 8.320993423461914\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 384, 12, 6)\n",
      "step: 0, train loss: 10.45010757446289, val loss: 10.450684547424316\n",
      "step: 50, train loss: 8.719573020935059, val loss: 8.733388900756836\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 384, 12, 10)\n",
      "step: 0, train loss: 10.415135383605957, val loss: 10.416749000549316\n",
      "step: 50, train loss: 8.2421236038208, val loss: 8.243412017822266\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 384, 12, 12)\n",
      "step: 0, train loss: 10.545541763305664, val loss: 10.550738334655762\n",
      "step: 50, train loss: 8.29870891571045, val loss: 8.285689353942871\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 512, 4, 6)\n",
      "step: 0, train loss: 10.526333808898926, val loss: 10.522470474243164\n",
      "step: 50, train loss: 7.7847394943237305, val loss: 7.796764373779297\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 512, 4, 10)\n",
      "step: 0, train loss: 10.503277778625488, val loss: 10.508216857910156\n",
      "step: 50, train loss: 7.6870551109313965, val loss: 7.653629779815674\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 512, 4, 12)\n",
      "step: 0, train loss: 10.434870719909668, val loss: 10.432538986206055\n",
      "step: 50, train loss: 7.5136003494262695, val loss: 7.5194573402404785\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 512, 8, 6)\n",
      "step: 0, train loss: 10.478498458862305, val loss: 10.478619575500488\n",
      "step: 50, train loss: 7.871967792510986, val loss: 7.851118564605713\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 512, 8, 10)\n",
      "step: 0, train loss: 10.473782539367676, val loss: 10.475183486938477\n",
      "step: 50, train loss: 7.70187520980835, val loss: 7.679257392883301\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 512, 8, 12)\n",
      "step: 0, train loss: 10.550623893737793, val loss: 10.557477951049805\n",
      "step: 50, train loss: 7.608982563018799, val loss: 7.6172051429748535\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 512, 12, 6)\n",
      "step: 0, train loss: 10.454814910888672, val loss: 10.450654029846191\n",
      "step: 50, train loss: 7.627552032470703, val loss: 7.641904830932617\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 512, 12, 10)\n",
      "step: 0, train loss: 10.54739761352539, val loss: 10.55189323425293\n",
      "step: 50, train loss: 7.671673774719238, val loss: 7.666480541229248\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 64, 200, 1e-05, 512, 12, 12)\n",
      "step: 0, train loss: 10.432960510253906, val loss: 10.429778099060059\n",
      "step: 50, train loss: 7.656111240386963, val loss: 7.647214889526367\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 256, 4, 6)\n",
      "step: 0, train loss: 10.372384071350098, val loss: 10.371628761291504\n",
      "step: 50, train loss: 2.7927520275115967, val loss: 2.7885477542877197\n",
      "step: 100, train loss: 2.619286298751831, val loss: 2.6342992782592773\n",
      "step: 150, train loss: 2.6266584396362305, val loss: 2.8150620460510254\n",
      "Final loss: 2.7810254096984863\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 256, 4, 10)\n",
      "step: 0, train loss: 10.379709243774414, val loss: 10.3809232711792\n",
      "step: 50, train loss: 2.738473415374756, val loss: 2.758847951889038\n",
      "step: 100, train loss: 2.6555237770080566, val loss: 2.6766011714935303\n",
      "step: 150, train loss: 2.6402530670166016, val loss: 2.6251604557037354\n",
      "Final loss: 2.4926226139068604\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 256, 4, 12)\n",
      "step: 0, train loss: 10.437154769897461, val loss: 10.435413360595703\n",
      "step: 50, train loss: 3.2047252655029297, val loss: 3.2615127563476562\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 256, 8, 6)\n",
      "step: 0, train loss: 10.429614067077637, val loss: 10.428628921508789\n",
      "step: 50, train loss: 3.141357421875, val loss: 3.1680784225463867\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 256, 8, 10)\n",
      "step: 0, train loss: 10.476140975952148, val loss: 10.476487159729004\n",
      "step: 50, train loss: 2.7526214122772217, val loss: 2.782700777053833\n",
      "step: 100, train loss: 2.648469924926758, val loss: 2.691518545150757\n",
      "step: 150, train loss: 2.618851900100708, val loss: 2.6268231868743896\n",
      "Final loss: 2.565546989440918\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 256, 8, 12)\n",
      "step: 0, train loss: 10.448153495788574, val loss: 10.451025009155273\n",
      "step: 50, train loss: 3.21622371673584, val loss: 3.213085889816284\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 256, 12, 6)\n",
      "step: 0, train loss: 10.424941062927246, val loss: 10.425934791564941\n",
      "step: 50, train loss: 2.8140790462493896, val loss: 2.781986713409424\n",
      "step: 100, train loss: 2.671846389770508, val loss: 2.6484432220458984\n",
      "step: 150, train loss: 2.639089345932007, val loss: 2.6247494220733643\n",
      "Final loss: 2.5829789638519287\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 256, 12, 10)\n",
      "step: 0, train loss: 10.4199857711792, val loss: 10.418834686279297\n",
      "step: 50, train loss: 2.8545000553131104, val loss: 2.789863348007202\n",
      "step: 100, train loss: 2.6289777755737305, val loss: 2.637716770172119\n",
      "step: 150, train loss: 2.624005079269409, val loss: 2.6610267162323\n",
      "Final loss: 2.555741548538208\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 256, 12, 12)\n",
      "step: 0, train loss: 10.446737289428711, val loss: 10.448738098144531\n",
      "step: 50, train loss: 2.7513349056243896, val loss: 2.7482452392578125\n",
      "step: 100, train loss: 2.6893911361694336, val loss: 2.660330295562744\n",
      "step: 150, train loss: 2.613967180252075, val loss: 2.659634590148926\n",
      "Final loss: 2.601118564605713\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 384, 4, 6)\n",
      "step: 0, train loss: 10.48354721069336, val loss: 10.47897720336914\n",
      "step: 50, train loss: 2.6619372367858887, val loss: 2.6812098026275635\n",
      "step: 100, train loss: 2.6395440101623535, val loss: 2.646158456802368\n",
      "step: 150, train loss: 2.6114141941070557, val loss: 2.604788303375244\n",
      "Final loss: 2.9809141159057617\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 384, 4, 10)\n",
      "step: 0, train loss: 10.438401222229004, val loss: 10.437094688415527\n",
      "step: 50, train loss: 3.1299383640289307, val loss: 3.173996686935425\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 384, 4, 12)\n",
      "step: 0, train loss: 10.510916709899902, val loss: 10.511624336242676\n",
      "step: 50, train loss: 3.2186336517333984, val loss: 3.1974120140075684\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 384, 8, 6)\n",
      "step: 0, train loss: 10.444815635681152, val loss: 10.440011024475098\n",
      "step: 50, train loss: 2.7989723682403564, val loss: 2.7378482818603516\n",
      "step: 100, train loss: 2.6177377700805664, val loss: 2.6329519748687744\n",
      "step: 150, train loss: 2.6046009063720703, val loss: 2.6813623905181885\n",
      "Final loss: 2.9560790061950684\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 384, 8, 10)\n",
      "step: 0, train loss: 10.461251258850098, val loss: 10.459107398986816\n",
      "step: 50, train loss: 3.23500919342041, val loss: 3.2444956302642822\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 384, 8, 12)\n",
      "step: 0, train loss: 10.412871360778809, val loss: 10.410855293273926\n",
      "step: 50, train loss: 3.208874464035034, val loss: 3.2233383655548096\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 384, 12, 6)\n",
      "step: 0, train loss: 10.42757797241211, val loss: 10.428971290588379\n",
      "step: 50, train loss: 2.713865280151367, val loss: 2.7132456302642822\n",
      "step: 100, train loss: 2.6379525661468506, val loss: 2.634291648864746\n",
      "step: 150, train loss: 2.5958447456359863, val loss: 2.6089913845062256\n",
      "Final loss: 2.6678826808929443\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 384, 12, 10)\n",
      "step: 0, train loss: 10.461503028869629, val loss: 10.460663795471191\n",
      "step: 50, train loss: 3.2445502281188965, val loss: 3.2567334175109863\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 384, 12, 12)\n",
      "step: 0, train loss: 10.466322898864746, val loss: 10.466469764709473\n",
      "step: 50, train loss: 3.2485289573669434, val loss: 3.2265377044677734\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 512, 4, 6)\n",
      "step: 0, train loss: 10.528739929199219, val loss: 10.531320571899414\n",
      "step: 50, train loss: 2.890856981277466, val loss: 2.9734606742858887\n",
      "step: 100, train loss: 2.810164451599121, val loss: 2.827596426010132\n",
      "step: 150, train loss: 2.799412727355957, val loss: 2.7701470851898193\n",
      "Final loss: 2.6957318782806396\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 512, 4, 10)\n",
      "step: 0, train loss: 10.43556022644043, val loss: 10.433387756347656\n",
      "step: 50, train loss: 3.257023572921753, val loss: 3.2585582733154297\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 512, 4, 12)\n",
      "step: 0, train loss: 10.38310432434082, val loss: 10.386053085327148\n",
      "step: 50, train loss: 3.2355706691741943, val loss: 3.225574016571045\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 512, 8, 6)\n",
      "step: 0, train loss: 10.406325340270996, val loss: 10.407090187072754\n",
      "step: 50, train loss: 2.713024377822876, val loss: 2.718658208847046\n",
      "step: 100, train loss: 2.6498401165008545, val loss: 2.661308526992798\n",
      "step: 150, train loss: 2.623614549636841, val loss: 2.6511242389678955\n",
      "Final loss: 3.0028302669525146\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 512, 8, 10)\n",
      "step: 0, train loss: 10.43509292602539, val loss: 10.43850326538086\n",
      "step: 50, train loss: 3.199633836746216, val loss: 3.2600674629211426\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 512, 8, 12)\n",
      "step: 0, train loss: 10.616421699523926, val loss: 10.614463806152344\n",
      "step: 50, train loss: 3.2103652954101562, val loss: 3.222256660461426\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 512, 12, 6)\n",
      "step: 0, train loss: 10.544461250305176, val loss: 10.546091079711914\n",
      "step: 50, train loss: 3.247774600982666, val loss: 3.2753190994262695\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 512, 12, 10)\n",
      "step: 0, train loss: 10.559107780456543, val loss: 10.555200576782227\n",
      "step: 50, train loss: 3.242058753967285, val loss: 3.240549087524414\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.001, 512, 12, 12)\n",
      "step: 0, train loss: 10.522061347961426, val loss: 10.52044677734375\n",
      "step: 50, train loss: 3.2318265438079834, val loss: 3.216693639755249\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 256, 4, 6)\n",
      "step: 0, train loss: 10.373374938964844, val loss: 10.372344970703125\n",
      "step: 50, train loss: 6.755669116973877, val loss: 6.736294746398926\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 256, 4, 10)\n",
      "step: 0, train loss: 10.370993614196777, val loss: 10.371456146240234\n",
      "step: 50, train loss: 6.774542808532715, val loss: 6.794476509094238\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 256, 4, 12)\n",
      "step: 0, train loss: 10.453155517578125, val loss: 10.454080581665039\n",
      "step: 50, train loss: 6.874333381652832, val loss: 6.876842021942139\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 256, 8, 6)\n",
      "step: 0, train loss: 10.45251750946045, val loss: 10.454402923583984\n",
      "step: 50, train loss: 6.900449752807617, val loss: 6.909682750701904\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 256, 8, 10)\n",
      "step: 0, train loss: 10.42879867553711, val loss: 10.427515983581543\n",
      "step: 50, train loss: 6.847421646118164, val loss: 6.8567280769348145\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 256, 8, 12)\n",
      "step: 0, train loss: 10.462787628173828, val loss: 10.464086532592773\n",
      "step: 50, train loss: 6.849424839019775, val loss: 6.856129169464111\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 256, 12, 6)\n",
      "step: 0, train loss: 10.446354866027832, val loss: 10.447745323181152\n",
      "step: 50, train loss: 6.842489719390869, val loss: 6.819730758666992\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 256, 12, 10)\n",
      "step: 0, train loss: 10.414972305297852, val loss: 10.412336349487305\n",
      "step: 50, train loss: 6.8880462646484375, val loss: 6.889996528625488\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 256, 12, 12)\n",
      "step: 0, train loss: 10.454839706420898, val loss: 10.453072547912598\n",
      "step: 50, train loss: 6.882782459259033, val loss: 6.882958889007568\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 384, 4, 6)\n",
      "step: 0, train loss: 10.421874046325684, val loss: 10.419940948486328\n",
      "step: 50, train loss: 5.257689952850342, val loss: 5.211813926696777\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 384, 4, 10)\n",
      "step: 0, train loss: 10.427672386169434, val loss: 10.428393363952637\n",
      "step: 50, train loss: 5.164997577667236, val loss: 5.143835067749023\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 384, 4, 12)\n",
      "step: 0, train loss: 10.5088529586792, val loss: 10.507964134216309\n",
      "step: 50, train loss: 5.1898627281188965, val loss: 5.194746017456055\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 384, 8, 6)\n",
      "step: 0, train loss: 10.407035827636719, val loss: 10.408616065979004\n",
      "step: 50, train loss: 5.151202201843262, val loss: 5.254372596740723\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 384, 8, 10)\n",
      "step: 0, train loss: 10.491231918334961, val loss: 10.494152069091797\n",
      "step: 50, train loss: 5.199414253234863, val loss: 5.2403178215026855\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 384, 8, 12)\n",
      "step: 0, train loss: 10.446541786193848, val loss: 10.447349548339844\n",
      "step: 50, train loss: 5.191604137420654, val loss: 5.17890739440918\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 384, 12, 6)\n",
      "step: 0, train loss: 10.482142448425293, val loss: 10.483735084533691\n",
      "step: 50, train loss: 5.135371208190918, val loss: 5.114513397216797\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 384, 12, 10)\n",
      "step: 0, train loss: 10.427504539489746, val loss: 10.426852226257324\n",
      "step: 50, train loss: 5.201058864593506, val loss: 5.1832685470581055\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 384, 12, 12)\n",
      "step: 0, train loss: 10.474109649658203, val loss: 10.4727201461792\n",
      "step: 50, train loss: 5.217958927154541, val loss: 5.212635517120361\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 512, 4, 6)\n",
      "step: 0, train loss: 10.528336524963379, val loss: 10.525015830993652\n",
      "step: 50, train loss: 4.042410373687744, val loss: 4.02059268951416\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 512, 4, 10)\n",
      "step: 0, train loss: 10.5935697555542, val loss: 10.598166465759277\n",
      "step: 50, train loss: 4.029864311218262, val loss: 4.032644271850586\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 512, 4, 12)\n",
      "step: 0, train loss: 10.353168487548828, val loss: 10.354432106018066\n",
      "step: 50, train loss: 4.016931056976318, val loss: 4.028751373291016\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 512, 8, 6)\n",
      "step: 0, train loss: 10.503047943115234, val loss: 10.504907608032227\n",
      "step: 50, train loss: 3.9801337718963623, val loss: 3.9900667667388916\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 512, 8, 10)\n",
      "step: 0, train loss: 10.610917091369629, val loss: 10.607901573181152\n",
      "step: 50, train loss: 4.05142068862915, val loss: 4.067892551422119\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 512, 8, 12)\n",
      "step: 0, train loss: 10.427984237670898, val loss: 10.426403045654297\n",
      "step: 50, train loss: 3.94040584564209, val loss: 4.025712966918945\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 512, 12, 6)\n",
      "step: 0, train loss: 10.404611587524414, val loss: 10.40461540222168\n",
      "step: 50, train loss: 3.989508867263794, val loss: 4.017318248748779\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 512, 12, 10)\n",
      "step: 0, train loss: 10.48681926727295, val loss: 10.489411354064941\n",
      "step: 50, train loss: 3.9973113536834717, val loss: 4.009945869445801\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 0.0001, 512, 12, 12)\n",
      "step: 0, train loss: 10.552433013916016, val loss: 10.55270004272461\n",
      "step: 50, train loss: 3.990373134613037, val loss: 3.9964118003845215\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 256, 4, 6)\n",
      "step: 0, train loss: 10.425397872924805, val loss: 10.427165985107422\n",
      "step: 50, train loss: 9.673696517944336, val loss: 9.67333698272705\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 256, 4, 10)\n",
      "step: 0, train loss: 10.431282997131348, val loss: 10.430493354797363\n",
      "step: 50, train loss: 9.281403541564941, val loss: 9.284492492675781\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 256, 4, 12)\n",
      "step: 0, train loss: 10.387044906616211, val loss: 10.390437126159668\n",
      "step: 50, train loss: 9.220196723937988, val loss: 9.186929702758789\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 256, 8, 6)\n",
      "step: 0, train loss: 10.451020240783691, val loss: 10.450935363769531\n",
      "step: 50, train loss: 9.686262130737305, val loss: 9.678535461425781\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 256, 8, 10)\n",
      "step: 0, train loss: 10.39653205871582, val loss: 10.394994735717773\n",
      "step: 50, train loss: 9.301665306091309, val loss: 9.315278053283691\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 256, 8, 12)\n",
      "step: 0, train loss: 10.429354667663574, val loss: 10.429698944091797\n",
      "step: 50, train loss: 9.250476837158203, val loss: 9.227015495300293\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 256, 12, 6)\n",
      "step: 0, train loss: 10.43075180053711, val loss: 10.430438041687012\n",
      "step: 50, train loss: 9.62044620513916, val loss: 9.61410140991211\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 256, 12, 10)\n",
      "step: 0, train loss: 10.44112777709961, val loss: 10.439785957336426\n",
      "step: 50, train loss: 9.300912857055664, val loss: 9.312911033630371\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 256, 12, 12)\n",
      "step: 0, train loss: 10.4575834274292, val loss: 10.455638885498047\n",
      "step: 50, train loss: 9.22652816772461, val loss: 9.226397514343262\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 384, 4, 6)\n",
      "step: 0, train loss: 10.477060317993164, val loss: 10.477997779846191\n",
      "step: 50, train loss: 8.618795394897461, val loss: 8.611467361450195\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 384, 4, 10)\n",
      "step: 0, train loss: 10.407833099365234, val loss: 10.406335830688477\n",
      "step: 50, train loss: 8.199624061584473, val loss: 8.215967178344727\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 384, 4, 12)\n",
      "step: 0, train loss: 10.451817512512207, val loss: 10.453563690185547\n",
      "step: 50, train loss: 8.379748344421387, val loss: 8.418743133544922\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 384, 8, 6)\n",
      "step: 0, train loss: 10.474337577819824, val loss: 10.474252700805664\n",
      "step: 50, train loss: 8.700231552124023, val loss: 8.706669807434082\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 384, 8, 10)\n",
      "step: 0, train loss: 10.421298027038574, val loss: 10.422194480895996\n",
      "step: 50, train loss: 8.268546104431152, val loss: 8.298864364624023\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 384, 8, 12)\n",
      "step: 0, train loss: 10.476143836975098, val loss: 10.474893569946289\n",
      "step: 50, train loss: 8.29753303527832, val loss: 8.28515338897705\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 384, 12, 6)\n",
      "step: 0, train loss: 10.521122932434082, val loss: 10.516800880432129\n",
      "step: 50, train loss: 8.771783828735352, val loss: 8.741959571838379\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 384, 12, 10)\n",
      "step: 0, train loss: 10.485461235046387, val loss: 10.487275123596191\n",
      "step: 50, train loss: 8.440166473388672, val loss: 8.419280052185059\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 384, 12, 12)\n",
      "step: 0, train loss: 10.46334457397461, val loss: 10.463953971862793\n",
      "step: 50, train loss: 8.268228530883789, val loss: 8.253584861755371\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 512, 4, 6)\n",
      "step: 0, train loss: 10.57702350616455, val loss: 10.580374717712402\n",
      "step: 50, train loss: 7.816861152648926, val loss: 7.856048583984375\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 512, 4, 10)\n",
      "step: 0, train loss: 10.384394645690918, val loss: 10.3842134475708\n",
      "step: 50, train loss: 7.511046886444092, val loss: 7.5326714515686035\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 512, 4, 12)\n",
      "step: 0, train loss: 10.531965255737305, val loss: 10.532419204711914\n",
      "step: 50, train loss: 7.597708702087402, val loss: 7.55983829498291\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 512, 8, 6)\n",
      "step: 0, train loss: 10.441259384155273, val loss: 10.43908405303955\n",
      "step: 50, train loss: 7.784964561462402, val loss: 7.826354503631592\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 512, 8, 10)\n",
      "step: 0, train loss: 10.550037384033203, val loss: 10.552077293395996\n",
      "step: 50, train loss: 7.489438533782959, val loss: 7.479783535003662\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 512, 8, 12)\n",
      "step: 0, train loss: 10.53223991394043, val loss: 10.532428741455078\n",
      "step: 50, train loss: 7.376097202301025, val loss: 7.38685417175293\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 512, 12, 6)\n",
      "step: 0, train loss: 10.405667304992676, val loss: 10.404993057250977\n",
      "step: 50, train loss: 7.740200042724609, val loss: 7.728546619415283\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 512, 12, 10)\n",
      "step: 0, train loss: 10.485698699951172, val loss: 10.489097595214844\n",
      "step: 50, train loss: 7.512313365936279, val loss: 7.555887222290039\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 128, 200, 1e-05, 512, 12, 12)\n",
      "step: 0, train loss: 10.487804412841797, val loss: 10.48898696899414\n",
      "step: 50, train loss: 7.480021953582764, val loss: 7.531662464141846\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 256, 4, 6)\n",
      "step: 0, train loss: 10.439617156982422, val loss: 10.439743995666504\n",
      "step: 50, train loss: 2.8400990962982178, val loss: 2.707658052444458\n",
      "step: 100, train loss: 2.638897657394409, val loss: 2.6473705768585205\n",
      "step: 150, train loss: 2.6090197563171387, val loss: 2.6350913047790527\n",
      "Final loss: 2.6066861152648926\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 256, 4, 10)\n",
      "step: 0, train loss: 10.401196479797363, val loss: 10.400550842285156\n",
      "step: 50, train loss: 2.813563585281372, val loss: 2.8329241275787354\n",
      "step: 100, train loss: 2.7197980880737305, val loss: 2.6650097370147705\n",
      "step: 150, train loss: 2.655503034591675, val loss: 2.6032111644744873\n",
      "Final loss: 2.632350444793701\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 256, 4, 12)\n",
      "step: 0, train loss: 10.395090103149414, val loss: 10.395913124084473\n",
      "step: 50, train loss: 3.0918571949005127, val loss: 3.262190580368042\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 256, 8, 6)\n",
      "step: 0, train loss: 10.412391662597656, val loss: 10.411426544189453\n",
      "step: 50, train loss: 2.723360300064087, val loss: 2.7423136234283447\n",
      "step: 100, train loss: 2.673227310180664, val loss: 2.6729366779327393\n",
      "step: 150, train loss: 2.6291985511779785, val loss: 2.6085898876190186\n",
      "Final loss: 2.6061785221099854\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 256, 8, 10)\n",
      "step: 0, train loss: 10.357697486877441, val loss: 10.358451843261719\n",
      "step: 50, train loss: 2.748013496398926, val loss: 2.735927104949951\n",
      "step: 100, train loss: 2.8045873641967773, val loss: 2.681046485900879\n",
      "step: 150, train loss: 2.660325050354004, val loss: 2.619406223297119\n",
      "Final loss: 2.946949005126953\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 256, 8, 12)\n",
      "step: 0, train loss: 10.408429145812988, val loss: 10.409154891967773\n",
      "step: 50, train loss: 2.9386305809020996, val loss: 2.9660468101501465\n",
      "step: 100, train loss: 2.702179193496704, val loss: 2.689528226852417\n",
      "step: 150, train loss: 2.65500545501709, val loss: 2.6306543350219727\n",
      "Final loss: 2.6008949279785156\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 256, 12, 6)\n",
      "step: 0, train loss: 10.447575569152832, val loss: 10.444316864013672\n",
      "step: 50, train loss: 2.7671871185302734, val loss: 2.698568344116211\n",
      "step: 100, train loss: 2.7121548652648926, val loss: 2.657759666442871\n",
      "step: 150, train loss: 2.6083524227142334, val loss: 2.6162776947021484\n",
      "Final loss: 2.5425825119018555\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 256, 12, 10)\n",
      "step: 0, train loss: 10.435654640197754, val loss: 10.435321807861328\n",
      "step: 50, train loss: 2.7718422412872314, val loss: 2.702951669692993\n",
      "step: 100, train loss: 2.7142651081085205, val loss: 2.660233736038208\n",
      "step: 150, train loss: 2.709958553314209, val loss: 2.6149957180023193\n",
      "Final loss: 2.5752358436584473\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 256, 12, 12)\n",
      "step: 0, train loss: 10.445317268371582, val loss: 10.444467544555664\n",
      "step: 50, train loss: 2.9018027782440186, val loss: 2.741677761077881\n",
      "step: 100, train loss: 2.6207456588745117, val loss: 2.6622889041900635\n",
      "step: 150, train loss: 2.6639657020568848, val loss: 2.631974458694458\n",
      "Final loss: 2.5627448558807373\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 384, 4, 6)\n",
      "step: 0, train loss: 10.4556884765625, val loss: 10.457040786743164\n",
      "step: 50, train loss: 2.655275821685791, val loss: 2.6995038986206055\n",
      "step: 100, train loss: 2.628669500350952, val loss: 2.6156845092773438\n",
      "step: 150, train loss: 2.7938320636749268, val loss: 2.6015920639038086\n",
      "Final loss: 2.6097397804260254\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 384, 4, 10)\n",
      "step: 0, train loss: 10.451251983642578, val loss: 10.4515380859375\n",
      "step: 50, train loss: 3.1798479557037354, val loss: 3.1623265743255615\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 384, 4, 12)\n",
      "step: 0, train loss: 10.457854270935059, val loss: 10.457891464233398\n",
      "step: 50, train loss: 3.2470781803131104, val loss: 3.2440531253814697\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 384, 8, 6)\n",
      "step: 0, train loss: 10.444086074829102, val loss: 10.442168235778809\n",
      "step: 50, train loss: 2.74247407913208, val loss: 2.6581902503967285\n",
      "step: 100, train loss: 2.616546630859375, val loss: 2.5991268157958984\n",
      "step: 150, train loss: 2.5954930782318115, val loss: 2.5707802772521973\n",
      "Final loss: 2.5700502395629883\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 384, 8, 10)\n",
      "step: 0, train loss: 10.510903358459473, val loss: 10.512110710144043\n",
      "step: 50, train loss: 3.2183215618133545, val loss: 3.2298696041107178\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 384, 8, 12)\n",
      "step: 0, train loss: 10.352770805358887, val loss: 10.352447509765625\n",
      "step: 50, train loss: 3.2199630737304688, val loss: 3.1859796047210693\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 384, 12, 6)\n",
      "step: 0, train loss: 10.444975852966309, val loss: 10.444819450378418\n",
      "step: 50, train loss: 2.654083013534546, val loss: 2.708143711090088\n",
      "step: 100, train loss: 2.6340866088867188, val loss: 2.642604112625122\n",
      "step: 150, train loss: 2.606097936630249, val loss: 2.6322262287139893\n",
      "Final loss: 2.567747116088867\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 384, 12, 10)\n",
      "step: 0, train loss: 10.437479019165039, val loss: 10.436176300048828\n",
      "step: 50, train loss: 3.1898374557495117, val loss: 3.228290319442749\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 384, 12, 12)\n",
      "step: 0, train loss: 10.447157859802246, val loss: 10.445429801940918\n",
      "step: 50, train loss: 3.208906888961792, val loss: 3.1973302364349365\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 512, 4, 6)\n",
      "step: 0, train loss: 10.454564094543457, val loss: 10.456607818603516\n",
      "step: 50, train loss: 3.2586121559143066, val loss: 3.1968276500701904\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 512, 4, 10)\n",
      "step: 0, train loss: 10.454437255859375, val loss: 10.4540376663208\n",
      "step: 50, train loss: 3.2061266899108887, val loss: 3.189459800720215\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 512, 4, 12)\n",
      "step: 0, train loss: 10.538061141967773, val loss: 10.537675857543945\n",
      "step: 50, train loss: 3.216920852661133, val loss: 3.207247257232666\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 512, 8, 6)\n",
      "step: 0, train loss: 10.537288665771484, val loss: 10.538948059082031\n",
      "step: 50, train loss: 2.8339550495147705, val loss: 2.793696880340576\n",
      "step: 100, train loss: 2.6453857421875, val loss: 2.656761407852173\n",
      "step: 150, train loss: 2.6326842308044434, val loss: 2.6336331367492676\n",
      "Final loss: 2.556729793548584\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 512, 8, 10)\n",
      "step: 0, train loss: 10.50616455078125, val loss: 10.502752304077148\n",
      "step: 50, train loss: 3.231748342514038, val loss: 3.2461953163146973\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 512, 8, 12)\n",
      "step: 0, train loss: 10.444706916809082, val loss: 10.446371078491211\n",
      "step: 50, train loss: 3.2203269004821777, val loss: 3.2065649032592773\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 512, 12, 6)\n",
      "step: 0, train loss: 10.486374855041504, val loss: 10.48376178741455\n",
      "step: 50, train loss: 3.264547109603882, val loss: 3.411641836166382\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 512, 12, 10)\n",
      "step: 0, train loss: 10.44618034362793, val loss: 10.446684837341309\n",
      "step: 50, train loss: 3.198824167251587, val loss: 3.191884994506836\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.001, 512, 12, 12)\n",
      "step: 0, train loss: 10.610465049743652, val loss: 10.610572814941406\n",
      "step: 50, train loss: 3.2153639793395996, val loss: 3.2106966972351074\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 256, 4, 6)\n",
      "step: 0, train loss: 10.4376802444458, val loss: 10.437326431274414\n",
      "step: 50, train loss: 6.858294486999512, val loss: 6.851292610168457\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 256, 4, 10)\n",
      "step: 0, train loss: 10.410635948181152, val loss: 10.40869426727295\n",
      "step: 50, train loss: 6.820148468017578, val loss: 6.798625946044922\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 256, 4, 12)\n",
      "step: 0, train loss: 10.444287300109863, val loss: 10.445765495300293\n",
      "step: 50, train loss: 6.824345588684082, val loss: 6.806512832641602\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 256, 8, 6)\n",
      "step: 0, train loss: 10.435302734375, val loss: 10.434420585632324\n",
      "step: 50, train loss: 6.870413303375244, val loss: 6.8780999183654785\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 256, 8, 10)\n",
      "step: 0, train loss: 10.39399528503418, val loss: 10.393485069274902\n",
      "step: 50, train loss: 6.813819408416748, val loss: 6.7985358238220215\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 256, 8, 12)\n",
      "step: 0, train loss: 10.432339668273926, val loss: 10.432541847229004\n",
      "step: 50, train loss: 6.826448917388916, val loss: 6.816390514373779\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 256, 12, 6)\n",
      "step: 0, train loss: 10.408217430114746, val loss: 10.41031265258789\n",
      "step: 50, train loss: 6.777413845062256, val loss: 6.786646842956543\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 256, 12, 10)\n",
      "step: 0, train loss: 10.45759391784668, val loss: 10.457928657531738\n",
      "step: 50, train loss: 6.755134105682373, val loss: 6.736294746398926\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 256, 12, 12)\n",
      "step: 0, train loss: 10.430954933166504, val loss: 10.42984390258789\n",
      "step: 50, train loss: 6.816742420196533, val loss: 6.815109729766846\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 384, 4, 6)\n",
      "step: 0, train loss: 10.476008415222168, val loss: 10.476597785949707\n",
      "step: 50, train loss: 5.271063804626465, val loss: 5.284046649932861\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 384, 4, 10)\n",
      "step: 0, train loss: 10.406998634338379, val loss: 10.409995079040527\n",
      "step: 50, train loss: 5.211513042449951, val loss: 5.176660537719727\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 384, 4, 12)\n",
      "step: 0, train loss: 10.463543891906738, val loss: 10.466085433959961\n",
      "step: 50, train loss: 5.038656711578369, val loss: 5.070924282073975\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 384, 8, 6)\n",
      "step: 0, train loss: 10.404333114624023, val loss: 10.407003402709961\n",
      "step: 50, train loss: 5.125504016876221, val loss: 5.114509105682373\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 384, 8, 10)\n",
      "step: 0, train loss: 10.492579460144043, val loss: 10.490488052368164\n",
      "step: 50, train loss: 5.2166643142700195, val loss: 5.253681659698486\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 384, 8, 12)\n",
      "step: 0, train loss: 10.473186492919922, val loss: 10.472297668457031\n",
      "step: 50, train loss: 5.17991828918457, val loss: 5.222169876098633\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 384, 12, 6)\n",
      "step: 0, train loss: 10.450176239013672, val loss: 10.448698997497559\n",
      "step: 50, train loss: 5.222178936004639, val loss: 5.243851184844971\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 384, 12, 10)\n",
      "step: 0, train loss: 10.430146217346191, val loss: 10.431270599365234\n",
      "step: 50, train loss: 5.089831352233887, val loss: 5.101496696472168\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 384, 12, 12)\n",
      "step: 0, train loss: 10.461426734924316, val loss: 10.463746070861816\n",
      "step: 50, train loss: 5.086381435394287, val loss: 5.092788219451904\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 512, 4, 6)\n",
      "step: 0, train loss: 10.489763259887695, val loss: 10.488443374633789\n",
      "step: 50, train loss: 3.889292001724243, val loss: 3.8736133575439453\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 512, 4, 10)\n",
      "step: 0, train loss: 10.410605430603027, val loss: 10.407110214233398\n",
      "step: 50, train loss: 3.9991676807403564, val loss: 4.018004894256592\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 512, 4, 12)\n",
      "step: 0, train loss: 10.557395935058594, val loss: 10.557302474975586\n",
      "step: 50, train loss: 4.071126937866211, val loss: 4.0073137283325195\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 512, 8, 6)\n",
      "step: 0, train loss: 10.451048851013184, val loss: 10.45508098602295\n",
      "step: 50, train loss: 3.9660439491271973, val loss: 3.9652087688446045\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 512, 8, 10)\n",
      "step: 0, train loss: 10.334083557128906, val loss: 10.333282470703125\n",
      "step: 50, train loss: 3.992128610610962, val loss: 3.9306182861328125\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 512, 8, 12)\n",
      "step: 0, train loss: 10.395712852478027, val loss: 10.395905494689941\n",
      "step: 50, train loss: 3.942864418029785, val loss: 4.023237228393555\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 512, 12, 6)\n",
      "step: 0, train loss: 10.452030181884766, val loss: 10.450152397155762\n",
      "step: 50, train loss: 3.8737294673919678, val loss: 3.9135830402374268\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 512, 12, 10)\n",
      "step: 0, train loss: 10.415482521057129, val loss: 10.41269302368164\n",
      "step: 50, train loss: 3.9768669605255127, val loss: 3.999626874923706\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 0.0001, 512, 12, 12)\n",
      "step: 0, train loss: 10.507001876831055, val loss: 10.503809928894043\n",
      "step: 50, train loss: 3.95198917388916, val loss: 3.9358408451080322\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 256, 4, 6)\n",
      "step: 0, train loss: 10.45610237121582, val loss: 10.455865859985352\n",
      "step: 50, train loss: 9.711807250976562, val loss: 9.717395782470703\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 256, 4, 10)\n",
      "step: 0, train loss: 10.4771089553833, val loss: 10.476056098937988\n",
      "step: 50, train loss: 9.4409818649292, val loss: 9.445903778076172\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 256, 4, 12)\n",
      "step: 0, train loss: 10.409612655639648, val loss: 10.410208702087402\n",
      "step: 50, train loss: 9.048234939575195, val loss: 9.040946960449219\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 256, 8, 6)\n",
      "step: 0, train loss: 10.460532188415527, val loss: 10.457878112792969\n",
      "step: 50, train loss: 9.649726867675781, val loss: 9.646340370178223\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 256, 8, 10)\n",
      "step: 0, train loss: 10.436296463012695, val loss: 10.436784744262695\n",
      "step: 50, train loss: 9.2715482711792, val loss: 9.284976959228516\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 256, 8, 12)\n",
      "step: 0, train loss: 10.450201034545898, val loss: 10.451868057250977\n",
      "step: 50, train loss: 9.196999549865723, val loss: 9.173707962036133\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 256, 12, 6)\n",
      "step: 0, train loss: 10.427106857299805, val loss: 10.428332328796387\n",
      "step: 50, train loss: 9.583498001098633, val loss: 9.567879676818848\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 256, 12, 10)\n",
      "step: 0, train loss: 10.440116882324219, val loss: 10.441688537597656\n",
      "step: 50, train loss: 9.332021713256836, val loss: 9.318842887878418\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 256, 12, 12)\n",
      "step: 0, train loss: 10.42840576171875, val loss: 10.429213523864746\n",
      "step: 50, train loss: 9.099142074584961, val loss: 9.108229637145996\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 384, 4, 6)\n",
      "step: 0, train loss: 10.485401153564453, val loss: 10.48617935180664\n",
      "step: 50, train loss: 8.672286033630371, val loss: 8.680728912353516\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 384, 4, 10)\n",
      "step: 0, train loss: 10.433602333068848, val loss: 10.432177543640137\n",
      "step: 50, train loss: 8.399751663208008, val loss: 8.410894393920898\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 384, 4, 12)\n",
      "step: 0, train loss: 10.439055442810059, val loss: 10.442187309265137\n",
      "step: 50, train loss: 8.1546049118042, val loss: 8.140105247497559\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 384, 8, 6)\n",
      "step: 0, train loss: 10.41425895690918, val loss: 10.413708686828613\n",
      "step: 50, train loss: 8.60886001586914, val loss: 8.617185592651367\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 384, 8, 10)\n",
      "step: 0, train loss: 10.436518669128418, val loss: 10.435227394104004\n",
      "step: 50, train loss: 8.258988380432129, val loss: 8.280861854553223\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 384, 8, 12)\n",
      "step: 0, train loss: 10.439092636108398, val loss: 10.438590049743652\n",
      "step: 50, train loss: 8.303861618041992, val loss: 8.308401107788086\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 384, 12, 6)\n",
      "step: 0, train loss: 10.394701957702637, val loss: 10.395195007324219\n",
      "step: 50, train loss: 8.732253074645996, val loss: 8.71303939819336\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 384, 12, 10)\n",
      "step: 0, train loss: 10.44012451171875, val loss: 10.442316055297852\n",
      "step: 50, train loss: 8.312646865844727, val loss: 8.321725845336914\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 384, 12, 12)\n",
      "step: 0, train loss: 10.415822982788086, val loss: 10.41573715209961\n",
      "step: 50, train loss: 8.327733039855957, val loss: 8.318931579589844\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 512, 4, 6)\n",
      "step: 0, train loss: 10.472044944763184, val loss: 10.471774101257324\n",
      "step: 50, train loss: 7.754031181335449, val loss: 7.754739284515381\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 512, 4, 10)\n",
      "step: 0, train loss: 10.490886688232422, val loss: 10.491283416748047\n",
      "step: 50, train loss: 7.432494640350342, val loss: 7.441561222076416\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 512, 4, 12)\n",
      "step: 0, train loss: 10.428732872009277, val loss: 10.432746887207031\n",
      "step: 50, train loss: 7.448019027709961, val loss: 7.446408748626709\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (32, 256, 200, 1e-05, 512, 8, 6)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 201\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_iters \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 201\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_iters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m         train_losses\u001b[38;5;241m.\u001b[39mappend(losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    203\u001b[0m         val_losses\u001b[38;5;241m.\u001b[39mappend(losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 167\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m(model, eval_iters)\u001b[0m\n\u001b[0;32m    165\u001b[0m         X, Y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[0;32m    166\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m model(X, Y)\n\u001b[1;32m--> 167\u001b[0m         losses[k] \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# Hyperparameter grid\n",
    "hyperparameter_grid = {\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'block_size': [64, 128, 256],\n",
    "    'max_iters': [200],\n",
    "    'learning_rate': [1e-3, 1e-4, 1e-5],\n",
    "    'n_embd': [256, 384, 512],\n",
    "    'n_head': [4, 8, 12],\n",
    "    'n_layer': [6, 10, 12],\n",
    "}\n",
    "dropout = 0.2\n",
    "eval_iters = 50\n",
    "\n",
    "# Function to create model with given hyperparameters\n",
    "def create_model(vocab_size, n_embd, n_head, n_layer):\n",
    "    class Head(nn.Module):\n",
    "        def __init__(self, head_size):\n",
    "            super().__init__()\n",
    "            self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            B, T, C = x.shape\n",
    "            k = self.key(x)\n",
    "            q = self.query(x)\n",
    "            wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "            wei = F.softmax(wei, dim=-1)\n",
    "            wei = self.dropout(wei)\n",
    "            v = self.value(x)\n",
    "            out = wei @ v\n",
    "            return out\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        def __init__(self, num_heads, head_size):\n",
    "            super().__init__()\n",
    "            self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "            self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "            out = self.dropout(self.proj(out))\n",
    "            return out\n",
    "\n",
    "    class FeedForward(nn.Module):\n",
    "        def __init__(self, n_embd):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_embd, 4 * n_embd),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4 * n_embd, n_embd),\n",
    "                nn.Dropout(dropout),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    class Block(nn.Module):\n",
    "        def __init__(self, n_embd, n_head):\n",
    "            super().__init__()\n",
    "            head_size = n_embd // n_head\n",
    "            self.sa = MultiHeadAttention(n_head, head_size)\n",
    "            self.ffws = FeedForward(n_embd)\n",
    "            self.ln1 = nn.LayerNorm(n_embd)\n",
    "            self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        def forward(self, x):\n",
    "            y = self.sa(x)\n",
    "            x = self.ln1(x + y)\n",
    "            y = self.ffws(x)\n",
    "            x = self.ln2(x + y)\n",
    "            return x\n",
    "\n",
    "    class GptLanguageModel(nn.Module):\n",
    "        def __init__(self, vocab_size):\n",
    "            super().__init__()\n",
    "            self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "            self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "            self.block = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "            self.ln_f = nn.LayerNorm(n_embd)\n",
    "            self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "            self.apply(self._init_weights)\n",
    "\n",
    "        def _init_weights(self, module):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        def forward(self, index, targets=None):\n",
    "            B, T = index.shape\n",
    "            tok_emb = self.token_embedding_table(index)\n",
    "            pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "            x = tok_emb + pos_emb\n",
    "            x = self.block(x)\n",
    "            x = self.ln_f(x)\n",
    "            logits = self.lm_head(x)\n",
    "\n",
    "            if targets is None:\n",
    "                loss = None\n",
    "            else:\n",
    "                B, T, C = logits.shape\n",
    "                logits = logits.view(B * T, C)\n",
    "                targets = targets.view(B * T)\n",
    "                loss = F.cross_entropy(logits, targets)\n",
    "            return logits, loss\n",
    "\n",
    "        def generate(self, index, max_new_tokens):\n",
    "            for _ in range(max_new_tokens):\n",
    "                logits, _ = self.forward(index)\n",
    "                logits = logits[:, -1, :]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                index_next = torch.multinomial(probs, num_samples=1)\n",
    "                index = torch.cat((index, index_next), dim=1)\n",
    "            return index\n",
    "\n",
    "    return GptLanguageModel(vocab_size).to(device)\n",
    "\n",
    "# Define data functions\n",
    "def get_random_chunk(split):\n",
    "    filename = \"train_split.txt\" if split == 'train' else \"val_split.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, file_size - block_size * batch_size)\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size * batch_size - 1)\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "    return data\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix]).to(device)\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix]).to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    return out\n",
    "\n",
    "# Load vocabulary\n",
    "chars = \"\"\n",
    "with open('vocab.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "string_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "# Initialize results\n",
    "best_loss = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Grid search over hyperparameters\n",
    "for params in itertools.product(*hyperparameter_grid.values()):\n",
    "    batch_size, block_size, max_iters, learning_rate, n_embd, n_head, n_layer = params\n",
    "    print(f\"\\nTesting combination: {params}\")\n",
    "    \n",
    "    model = create_model(vocab_size, n_embd, n_head, n_layer)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    skip_combination = False\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "        if iter % eval_iters == 0:\n",
    "            losses = estimate_loss(model, eval_iters)\n",
    "            train_losses.append(losses['train'])\n",
    "            val_losses.append(losses['val'])\n",
    "            print(f\"step: {iter}, train loss: {losses['train']}, val loss: {losses['val']}\")\n",
    "\n",
    "            if iter ==  eval_iters and losses['train'] > 3:\n",
    "                print(\"Skipping to next combination due to high train loss.\")\n",
    "                skip_combination = True\n",
    "                break\n",
    "\n",
    "        xb, yb = get_batch('train')\n",
    "        logits, loss = model.forward(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if skip_combination:\n",
    "        continue\n",
    "\n",
    "    print(f\"Final loss: {loss.item()}\")\n",
    "    \n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        best_params = {\n",
    "            'batch_size': batch_size,\n",
    "            'block_size': block_size,\n",
    "            'max_iters': max_iters,\n",
    "            'learning_rate': learning_rate,\n",
    "            'n_embd': n_embd,\n",
    "            'n_head': n_head,\n",
    "            'n_layer': n_layer\n",
    "        }\n",
    "\n",
    "# Save best parameters\n",
    "with open('best_params.pkl', 'wb') as f:\n",
    "    pickle.dump(best_params, f)\n",
    "\n",
    "print(f\"Best parameters: {best_params} with loss: {best_loss}\")\n",
    "\n",
    "# Plotting losses\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss') \n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ae3705-3833-4fc9-8582-cf5d681fa63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Vocabulary size: 32171\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 256, 4, 6)\n",
      "step: 0, train loss: 10.437539100646973, val loss: 10.436705589294434\n",
      "step: 50, train loss: 2.767430067062378, val loss: 2.7613794803619385\n",
      "step: 100, train loss: 2.755141258239746, val loss: 2.7092654705047607\n",
      "step: 150, train loss: 2.510037899017334, val loss: 2.526036024093628\n",
      "Final loss: 2.4095523357391357\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 256, 4, 10)\n",
      "step: 0, train loss: 10.437516212463379, val loss: 10.439190864562988\n",
      "step: 50, train loss: 3.1073923110961914, val loss: 3.1381707191467285\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 256, 4, 12)\n",
      "step: 0, train loss: 10.43768310546875, val loss: 10.437600135803223\n",
      "step: 50, train loss: 3.0319368839263916, val loss: 3.0540802478790283\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 256, 8, 6)\n",
      "step: 0, train loss: 10.434077262878418, val loss: 10.436534881591797\n",
      "step: 50, train loss: 2.8236184120178223, val loss: 2.815577983856201\n",
      "step: 100, train loss: 2.6559343338012695, val loss: 2.6353211402893066\n",
      "step: 150, train loss: 2.5002334117889404, val loss: 2.5119736194610596\n",
      "Final loss: 2.2787601947784424\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 256, 8, 10)\n",
      "step: 0, train loss: 10.463257789611816, val loss: 10.463025093078613\n",
      "step: 50, train loss: 2.795832872390747, val loss: 2.8764636516571045\n",
      "step: 100, train loss: 2.6519696712493896, val loss: 2.8015871047973633\n",
      "step: 150, train loss: 2.6213669776916504, val loss: 2.6551313400268555\n",
      "Final loss: 2.3893232345581055\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 256, 8, 12)\n",
      "step: 0, train loss: 10.412972450256348, val loss: 10.411494255065918\n",
      "step: 50, train loss: 3.2058229446411133, val loss: 3.2320022583007812\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 256, 12, 6)\n",
      "step: 0, train loss: 10.426374435424805, val loss: 10.424443244934082\n",
      "step: 50, train loss: 2.773138999938965, val loss: 2.727530002593994\n",
      "step: 100, train loss: 2.6499292850494385, val loss: 2.6366424560546875\n",
      "step: 150, train loss: 2.6058363914489746, val loss: 2.600062608718872\n",
      "Final loss: 2.475923538208008\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 256, 12, 10)\n",
      "step: 0, train loss: 10.41525650024414, val loss: 10.410945892333984\n",
      "step: 50, train loss: 2.8745672702789307, val loss: 2.8050332069396973\n",
      "step: 100, train loss: 2.675299644470215, val loss: 2.764946699142456\n",
      "step: 150, train loss: 2.616029739379883, val loss: 2.5786542892456055\n",
      "Final loss: 2.348454713821411\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 256, 12, 12)\n",
      "step: 0, train loss: 10.45495319366455, val loss: 10.453912734985352\n",
      "step: 50, train loss: 2.7887024879455566, val loss: 2.789128303527832\n",
      "step: 100, train loss: 2.6965301036834717, val loss: 2.7043862342834473\n",
      "step: 150, train loss: 2.5399587154388428, val loss: 2.5746564865112305\n",
      "Final loss: 2.2998363971710205\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 384, 4, 6)\n",
      "step: 0, train loss: 10.504863739013672, val loss: 10.504524230957031\n",
      "step: 50, train loss: 2.6970834732055664, val loss: 2.6777544021606445\n",
      "step: 100, train loss: 2.6743850708007812, val loss: 2.7081236839294434\n",
      "step: 150, train loss: 2.567350149154663, val loss: 2.60162353515625\n",
      "Final loss: 2.4587345123291016\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 384, 4, 10)\n",
      "step: 0, train loss: 10.43742561340332, val loss: 10.439920425415039\n",
      "step: 50, train loss: 3.1775803565979004, val loss: 3.212656259536743\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 384, 4, 12)\n",
      "step: 0, train loss: 10.439470291137695, val loss: 10.438465118408203\n",
      "step: 50, train loss: 3.2664849758148193, val loss: 3.2985479831695557\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 384, 8, 6)\n",
      "step: 0, train loss: 10.47231674194336, val loss: 10.472454071044922\n",
      "step: 50, train loss: 2.737757921218872, val loss: 2.698856830596924\n",
      "step: 100, train loss: 2.6370606422424316, val loss: 2.678617238998413\n",
      "step: 150, train loss: 2.6260833740234375, val loss: 2.6006088256835938\n",
      "Final loss: 2.4456400871276855\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 384, 8, 10)\n",
      "step: 0, train loss: 10.453143119812012, val loss: 10.451682090759277\n",
      "step: 50, train loss: 3.2348601818084717, val loss: 3.2270593643188477\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 384, 8, 12)\n",
      "step: 0, train loss: 10.484580993652344, val loss: 10.483654022216797\n",
      "step: 50, train loss: 3.238480567932129, val loss: 3.2095694541931152\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 384, 12, 6)\n",
      "step: 0, train loss: 10.459453582763672, val loss: 10.458189964294434\n",
      "step: 50, train loss: 2.6621875762939453, val loss: 2.921605110168457\n",
      "step: 100, train loss: 2.626469135284424, val loss: 2.593522548675537\n",
      "step: 150, train loss: 2.49259877204895, val loss: 2.512223243713379\n",
      "Final loss: 2.591872453689575\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 384, 12, 10)\n",
      "step: 0, train loss: 10.49268913269043, val loss: 10.488387107849121\n",
      "step: 50, train loss: 3.2142674922943115, val loss: 3.2020623683929443\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 384, 12, 12)\n",
      "step: 0, train loss: 10.51791000366211, val loss: 10.520694732666016\n",
      "step: 50, train loss: 3.256678581237793, val loss: 3.348411798477173\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 512, 4, 6)\n",
      "step: 0, train loss: 10.458841323852539, val loss: 10.459046363830566\n",
      "step: 50, train loss: 3.315180778503418, val loss: 3.214400291442871\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 512, 4, 10)\n",
      "step: 0, train loss: 10.478996276855469, val loss: 10.481013298034668\n",
      "step: 50, train loss: 3.250852108001709, val loss: 3.277902126312256\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 512, 4, 12)\n",
      "step: 0, train loss: 10.443509101867676, val loss: 10.445024490356445\n",
      "step: 50, train loss: 3.210904836654663, val loss: 3.2457327842712402\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 512, 8, 6)\n",
      "step: 0, train loss: 10.539664268493652, val loss: 10.543414115905762\n",
      "step: 50, train loss: 3.2573578357696533, val loss: 3.2090530395507812\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 512, 8, 10)\n",
      "step: 0, train loss: 10.531580924987793, val loss: 10.531700134277344\n",
      "step: 50, train loss: 3.2016382217407227, val loss: 3.215336799621582\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 512, 8, 12)\n",
      "step: 0, train loss: 10.458525657653809, val loss: 10.459421157836914\n",
      "step: 50, train loss: 3.2944536209106445, val loss: 3.26558256149292\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 512, 12, 6)\n",
      "step: 0, train loss: 10.576987266540527, val loss: 10.57482624053955\n",
      "step: 50, train loss: 3.2232840061187744, val loss: 3.203382968902588\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 512, 12, 10)\n",
      "step: 0, train loss: 10.375931739807129, val loss: 10.378085136413574\n",
      "step: 50, train loss: 3.220522403717041, val loss: 3.2110440731048584\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 64, 200, 0.001, 512, 12, 12)\n",
      "step: 0, train loss: 10.520354270935059, val loss: 10.520855903625488\n",
      "step: 50, train loss: 3.2265515327453613, val loss: 3.2146353721618652\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 256, 4, 6)\n",
      "step: 0, train loss: 10.400971412658691, val loss: 10.402283668518066\n",
      "step: 50, train loss: 2.7396271228790283, val loss: 2.7349398136138916\n",
      "step: 100, train loss: 2.6602628231048584, val loss: 2.6534390449523926\n",
      "step: 150, train loss: 2.647965669631958, val loss: 2.639744520187378\n",
      "Final loss: 2.570838212966919\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 256, 4, 10)\n",
      "step: 0, train loss: 10.417156219482422, val loss: 10.417464256286621\n",
      "step: 50, train loss: 2.71401309967041, val loss: 2.7199296951293945\n",
      "step: 100, train loss: 2.642632484436035, val loss: 2.6264150142669678\n",
      "step: 150, train loss: 2.604691982269287, val loss: 2.5965781211853027\n",
      "Final loss: 2.4516985416412354\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 256, 4, 12)\n",
      "step: 0, train loss: 10.46334457397461, val loss: 10.461862564086914\n",
      "step: 50, train loss: 2.956310749053955, val loss: 2.9013595581054688\n",
      "step: 100, train loss: 2.667234420776367, val loss: 2.7759933471679688\n",
      "step: 150, train loss: 2.6476986408233643, val loss: 2.63759183883667\n",
      "Final loss: 2.799854040145874\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 256, 8, 6)\n",
      "step: 0, train loss: 10.429206848144531, val loss: 10.426661491394043\n",
      "step: 50, train loss: 2.702205181121826, val loss: 2.698244333267212\n",
      "step: 100, train loss: 2.587116003036499, val loss: 2.6511669158935547\n",
      "step: 150, train loss: 2.5781846046447754, val loss: 2.59063720703125\n",
      "Final loss: 2.4815471172332764\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 256, 8, 10)\n",
      "step: 0, train loss: 10.447531700134277, val loss: 10.446420669555664\n",
      "step: 50, train loss: 2.9510059356689453, val loss: 2.9458136558532715\n",
      "step: 100, train loss: 2.664799213409424, val loss: 2.6796023845672607\n",
      "step: 150, train loss: 2.6734237670898438, val loss: 2.6665053367614746\n",
      "Final loss: 2.513622283935547\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 256, 8, 12)\n",
      "step: 0, train loss: 10.449934005737305, val loss: 10.449984550476074\n",
      "step: 50, train loss: 2.7255587577819824, val loss: 2.7083370685577393\n",
      "step: 100, train loss: 2.636653423309326, val loss: 2.653069496154785\n",
      "step: 150, train loss: 2.569077253341675, val loss: 2.55119252204895\n",
      "Final loss: 2.518256187438965\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 256, 12, 6)\n",
      "step: 0, train loss: 10.417703628540039, val loss: 10.416165351867676\n",
      "step: 50, train loss: 2.7712554931640625, val loss: 2.7581536769866943\n",
      "step: 100, train loss: 2.6670455932617188, val loss: 2.63566517829895\n",
      "step: 150, train loss: 2.588073968887329, val loss: 2.7860803604125977\n",
      "Final loss: 2.598550796508789\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 256, 12, 10)\n",
      "step: 0, train loss: 10.43981647491455, val loss: 10.439977645874023\n",
      "step: 50, train loss: 2.712794542312622, val loss: 2.740349531173706\n",
      "step: 100, train loss: 2.623504400253296, val loss: 2.634585380554199\n",
      "step: 150, train loss: 2.611726760864258, val loss: 2.6077117919921875\n",
      "Final loss: 2.520155906677246\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 256, 12, 12)\n",
      "step: 0, train loss: 10.426336288452148, val loss: 10.426143646240234\n",
      "step: 50, train loss: 2.7738094329833984, val loss: 2.7737457752227783\n",
      "step: 100, train loss: 2.634859561920166, val loss: 2.635856866836548\n",
      "step: 150, train loss: 2.626178026199341, val loss: 2.59458065032959\n",
      "Final loss: 2.5383641719818115\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 384, 4, 6)\n",
      "step: 0, train loss: 10.439749717712402, val loss: 10.442795753479004\n",
      "step: 50, train loss: 2.710638999938965, val loss: 2.7218191623687744\n",
      "step: 100, train loss: 2.6475424766540527, val loss: 2.6596169471740723\n",
      "step: 150, train loss: 2.5969693660736084, val loss: 2.5883283615112305\n",
      "Final loss: 2.454895496368408\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 384, 4, 10)\n",
      "step: 0, train loss: 10.423397064208984, val loss: 10.42361831665039\n",
      "step: 50, train loss: 3.212226629257202, val loss: 3.201821804046631\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 384, 4, 12)\n",
      "step: 0, train loss: 10.52277946472168, val loss: 10.525611877441406\n",
      "step: 50, train loss: 3.2855064868927, val loss: 3.1976747512817383\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 384, 8, 6)\n",
      "step: 0, train loss: 10.424572944641113, val loss: 10.42464828491211\n",
      "step: 50, train loss: 2.707751512527466, val loss: 2.695005416870117\n",
      "step: 100, train loss: 2.6729483604431152, val loss: 2.6390326023101807\n",
      "step: 150, train loss: 2.582355260848999, val loss: 2.5965118408203125\n",
      "Final loss: 2.4518728256225586\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 384, 8, 10)\n",
      "step: 0, train loss: 10.483315467834473, val loss: 10.4816255569458\n",
      "step: 50, train loss: 3.2166717052459717, val loss: 3.215900182723999\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 384, 8, 12)\n",
      "step: 0, train loss: 10.493197441101074, val loss: 10.493027687072754\n",
      "step: 50, train loss: 3.240898847579956, val loss: 3.234360456466675\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 384, 12, 6)\n",
      "step: 0, train loss: 10.48486042022705, val loss: 10.488042831420898\n",
      "step: 50, train loss: 2.6785004138946533, val loss: 2.6650049686431885\n",
      "step: 100, train loss: 2.6424953937530518, val loss: 2.6043202877044678\n",
      "step: 150, train loss: 2.5914885997772217, val loss: 2.5884649753570557\n",
      "Final loss: 2.5234718322753906\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 384, 12, 10)\n",
      "step: 0, train loss: 10.496150016784668, val loss: 10.49463176727295\n",
      "step: 50, train loss: 3.07623291015625, val loss: 3.0772910118103027\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 384, 12, 12)\n",
      "step: 0, train loss: 10.47198486328125, val loss: 10.470547676086426\n",
      "step: 50, train loss: 3.2220635414123535, val loss: 3.2712292671203613\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 512, 4, 6)\n",
      "step: 0, train loss: 10.49951457977295, val loss: 10.501866340637207\n",
      "step: 50, train loss: 3.2283694744110107, val loss: 3.222245693206787\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 512, 4, 10)\n",
      "step: 0, train loss: 10.669463157653809, val loss: 10.669846534729004\n",
      "step: 50, train loss: 3.203958511352539, val loss: 3.2491517066955566\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 512, 4, 12)\n",
      "step: 0, train loss: 10.419049263000488, val loss: 10.422940254211426\n",
      "step: 50, train loss: 3.220639228820801, val loss: 3.2205307483673096\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 512, 8, 6)\n",
      "step: 0, train loss: 10.531518936157227, val loss: 10.530599594116211\n",
      "step: 50, train loss: 2.8768608570098877, val loss: 2.867672920227051\n",
      "step: 100, train loss: 2.678645610809326, val loss: 2.6631600856781006\n",
      "step: 150, train loss: 2.6405255794525146, val loss: 2.6462771892547607\n",
      "Final loss: 3.0185351371765137\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 512, 8, 10)\n",
      "step: 0, train loss: 10.475139617919922, val loss: 10.473367691040039\n",
      "step: 50, train loss: 3.23244571685791, val loss: 3.290241003036499\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 512, 8, 12)\n",
      "step: 0, train loss: 10.433480262756348, val loss: 10.436139106750488\n",
      "step: 50, train loss: 3.261519432067871, val loss: 3.195180892944336\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 512, 12, 6)\n",
      "step: 0, train loss: 10.507046699523926, val loss: 10.504962921142578\n",
      "step: 50, train loss: 2.8155829906463623, val loss: 2.8157784938812256\n",
      "step: 100, train loss: 2.621640682220459, val loss: 2.6580982208251953\n",
      "step: 150, train loss: 2.616673231124878, val loss: 2.599538803100586\n",
      "Final loss: 4.3121113777160645\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 512, 12, 10)\n",
      "step: 0, train loss: 10.41552448272705, val loss: 10.412700653076172\n",
      "step: 50, train loss: 3.212949752807617, val loss: 3.215548515319824\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 128, 200, 0.001, 512, 12, 12)\n",
      "step: 0, train loss: 10.479277610778809, val loss: 10.4801664352417\n",
      "step: 50, train loss: 3.211153507232666, val loss: 3.21614146232605\n",
      "Skipping to next combination due to high train loss.\n",
      "\n",
      "Testing combination: (64, 256, 200, 0.001, 256, 4, 6)\n",
      "step: 0, train loss: 10.452040672302246, val loss: 10.452681541442871\n",
      "step: 50, train loss: 2.6601390838623047, val loss: 2.719588279724121\n",
      "step: 100, train loss: 2.6332502365112305, val loss: 2.614119052886963\n",
      "step: 150, train loss: 2.5728228092193604, val loss: 2.615706443786621\n",
      "Final loss: 2.569420576095581\n",
      "\n",
      "Testing combination: (64, 256, 200, 0.001, 256, 4, 10)\n",
      "step: 0, train loss: 10.471029281616211, val loss: 10.470305442810059\n",
      "step: 50, train loss: 2.727616548538208, val loss: 2.750234365463257\n",
      "step: 100, train loss: 2.695003032684326, val loss: 2.607245445251465\n",
      "step: 150, train loss: 2.603109836578369, val loss: 2.6028172969818115\n",
      "Final loss: 2.5669522285461426\n",
      "\n",
      "Testing combination: (64, 256, 200, 0.001, 256, 4, 12)\n",
      "step: 0, train loss: 10.392831802368164, val loss: 10.393087387084961\n",
      "step: 50, train loss: 2.9990954399108887, val loss: 2.9535980224609375\n",
      "step: 100, train loss: 2.7725718021392822, val loss: 2.7733473777770996\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 214\u001b[0m\n\u001b[0;32m    212\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(xb, yb)\n\u001b[0;32m    213\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 214\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_combination:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# Hyperparameter grid\n",
    "hyperparameter_grid = {\n",
    "    'batch_size': [64],\n",
    "    'block_size': [64, 128, 256],\n",
    "    'max_iters': [200],\n",
    "    'learning_rate': [1e-3],\n",
    "    'n_embd': [256, 384, 512],\n",
    "    'n_head': [4, 8, 12],\n",
    "    'n_layer': [6, 10, 12],\n",
    "}\n",
    "dropout = 0.2\n",
    "eval_iters = 50\n",
    "\n",
    "# Function to create model with given hyperparameters\n",
    "def create_model(vocab_size, n_embd, n_head, n_layer):\n",
    "    class Head(nn.Module):\n",
    "        def __init__(self, head_size):\n",
    "            super().__init__()\n",
    "            self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            B, T, C = x.shape\n",
    "            k = self.key(x)\n",
    "            q = self.query(x)\n",
    "            wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "            wei = F.softmax(wei, dim=-1)\n",
    "            wei = self.dropout(wei)\n",
    "            v = self.value(x)\n",
    "            out = wei @ v\n",
    "            return out\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        def __init__(self, num_heads, head_size):\n",
    "            super().__init__()\n",
    "            self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "            self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "            out = self.dropout(self.proj(out))\n",
    "            return out\n",
    "\n",
    "    class FeedForward(nn.Module):\n",
    "        def __init__(self, n_embd):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_embd, 4 * n_embd),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4 * n_embd, n_embd),\n",
    "                nn.Dropout(dropout),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    class Block(nn.Module):\n",
    "        def __init__(self, n_embd, n_head):\n",
    "            super().__init__()\n",
    "            head_size = n_embd // n_head\n",
    "            self.sa = MultiHeadAttention(n_head, head_size)\n",
    "            self.ffws = FeedForward(n_embd)\n",
    "            self.ln1 = nn.LayerNorm(n_embd)\n",
    "            self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        def forward(self, x):\n",
    "            y = self.sa(x)\n",
    "            x = self.ln1(x + y)\n",
    "            y = self.ffws(x)\n",
    "            x = self.ln2(x + y)\n",
    "            return x\n",
    "\n",
    "    class GptLanguageModel(nn.Module):\n",
    "        def __init__(self, vocab_size):\n",
    "            super().__init__()\n",
    "            self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "            self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "            self.block = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "            self.ln_f = nn.LayerNorm(n_embd)\n",
    "            self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "            self.apply(self._init_weights)\n",
    "\n",
    "        def _init_weights(self, module):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        def forward(self, index, targets=None):\n",
    "            B, T = index.shape\n",
    "            tok_emb = self.token_embedding_table(index)\n",
    "            pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "            x = tok_emb + pos_emb\n",
    "            x = self.block(x)\n",
    "            x = self.ln_f(x)\n",
    "            logits = self.lm_head(x)\n",
    "\n",
    "            if targets is None:\n",
    "                loss = None\n",
    "            else:\n",
    "                B, T, C = logits.shape\n",
    "                logits = logits.view(B * T, C)\n",
    "                targets = targets.view(B * T)\n",
    "                loss = F.cross_entropy(logits, targets)\n",
    "            return logits, loss\n",
    "\n",
    "        def generate(self, index, max_new_tokens):\n",
    "            for _ in range(max_new_tokens):\n",
    "                logits, _ = self.forward(index)\n",
    "                logits = logits[:, -1, :]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                index_next = torch.multinomial(probs, num_samples=1)\n",
    "                index = torch.cat((index, index_next), dim=1)\n",
    "            return index\n",
    "\n",
    "    return GptLanguageModel(vocab_size).to(device)\n",
    "\n",
    "# Define data functions\n",
    "def get_random_chunk(split):\n",
    "    filename = \"train_split.txt\" if split == 'train' else \"val_split.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, file_size - block_size * batch_size)\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size * batch_size - 1)\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "    return data\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix]).to(device)\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix]).to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    return out\n",
    "\n",
    "# Load vocabulary\n",
    "chars = \"\"\n",
    "with open('vocab.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "string_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "# Initialize results\n",
    "best_loss = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Grid search over hyperparameters\n",
    "for params in itertools.product(*hyperparameter_grid.values()):\n",
    "    batch_size, block_size, max_iters, learning_rate, n_embd, n_head, n_layer = params\n",
    "    print(f\"\\nTesting combination: {params}\")\n",
    "    \n",
    "    model = create_model(vocab_size, n_embd, n_head, n_layer)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    skip_combination = False\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "        if iter % eval_iters == 0:\n",
    "            losses = estimate_loss(model, eval_iters)\n",
    "            train_losses.append(losses['train'])\n",
    "            val_losses.append(losses['val'])\n",
    "            print(f\"step: {iter}, train loss: {losses['train']}, val loss: {losses['val']}\")\n",
    "\n",
    "            if iter ==  eval_iters and losses['train'] > 3:\n",
    "                print(\"Skipping to next combination due to high train loss.\")\n",
    "                skip_combination = True\n",
    "                break\n",
    "\n",
    "        xb, yb = get_batch('train')\n",
    "        logits, loss = model.forward(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if skip_combination:\n",
    "        continue\n",
    "\n",
    "    print(f\"Final loss: {loss.item()}\")\n",
    "    \n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        best_params = {\n",
    "            'batch_size': batch_size,\n",
    "            'block_size': block_size,\n",
    "            'max_iters': max_iters,\n",
    "            'learning_rate': learning_rate,\n",
    "            'n_embd': n_embd,\n",
    "            'n_head': n_head,\n",
    "            'n_layer': n_layer\n",
    "        }\n",
    "\n",
    "# Save best parameters\n",
    "with open('best_params.pkl', 'wb') as f:\n",
    "    pickle.dump(best_params, f)\n",
    "\n",
    "print(f\"Best parameters: {best_params} with loss: {best_loss}\")\n",
    "\n",
    "# Plotting losses\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss') \n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c76b9af9-5c15-4a85-80b1-82533d13f332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Vocabulary size: 32171\n",
      "\n",
      "Testing combination: (64, 256, 200, 0.001, 256, 4, 6)\n",
      "step: 0, train loss: 10.452496528625488, val loss: 10.453695297241211\n",
      "step: 50, train loss: 2.702472448348999, val loss: 2.698342800140381\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 214\u001b[0m\n\u001b[0;32m    212\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(xb, yb)\n\u001b[0;32m    213\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 214\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_combination:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# Hyperparameter grid\n",
    "hyperparameter_grid = {\n",
    "    'batch_size': [64],\n",
    "    'block_size': [256],\n",
    "    'max_iters': [200],\n",
    "    'learning_rate': [1e-3],\n",
    "    'n_embd': [256, 384, 512],\n",
    "    'n_head': [4, 8, 12],\n",
    "    'n_layer': [6, 10, 12],\n",
    "}\n",
    "dropout = 0.2\n",
    "eval_iters = 50\n",
    "\n",
    "# Function to create model with given hyperparameters\n",
    "def create_model(vocab_size, n_embd, n_head, n_layer):\n",
    "    class Head(nn.Module):\n",
    "        def __init__(self, head_size):\n",
    "            super().__init__()\n",
    "            self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            B, T, C = x.shape\n",
    "            k = self.key(x)\n",
    "            q = self.query(x)\n",
    "            wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "            wei = F.softmax(wei, dim=-1)\n",
    "            wei = self.dropout(wei)\n",
    "            v = self.value(x)\n",
    "            out = wei @ v\n",
    "            return out\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        def __init__(self, num_heads, head_size):\n",
    "            super().__init__()\n",
    "            self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "            self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "            out = self.dropout(self.proj(out))\n",
    "            return out\n",
    "\n",
    "    class FeedForward(nn.Module):\n",
    "        def __init__(self, n_embd):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_embd, 4 * n_embd),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4 * n_embd, n_embd),\n",
    "                nn.Dropout(dropout),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    class Block(nn.Module):\n",
    "        def __init__(self, n_embd, n_head):\n",
    "            super().__init__()\n",
    "            head_size = n_embd // n_head\n",
    "            self.sa = MultiHeadAttention(n_head, head_size)\n",
    "            self.ffws = FeedForward(n_embd)\n",
    "            self.ln1 = nn.LayerNorm(n_embd)\n",
    "            self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        def forward(self, x):\n",
    "            y = self.sa(x)\n",
    "            x = self.ln1(x + y)\n",
    "            y = self.ffws(x)\n",
    "            x = self.ln2(x + y)\n",
    "            return x\n",
    "\n",
    "    class GptLanguageModel(nn.Module):\n",
    "        def __init__(self, vocab_size):\n",
    "            super().__init__()\n",
    "            self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "            self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "            self.block = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "            self.ln_f = nn.LayerNorm(n_embd)\n",
    "            self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "            self.apply(self._init_weights)\n",
    "\n",
    "        def _init_weights(self, module):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        def forward(self, index, targets=None):\n",
    "            B, T = index.shape\n",
    "            tok_emb = self.token_embedding_table(index)\n",
    "            pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "            x = tok_emb + pos_emb\n",
    "            x = self.block(x)\n",
    "            x = self.ln_f(x)\n",
    "            logits = self.lm_head(x)\n",
    "\n",
    "            if targets is None:\n",
    "                loss = None\n",
    "            else:\n",
    "                B, T, C = logits.shape\n",
    "                logits = logits.view(B * T, C)\n",
    "                targets = targets.view(B * T)\n",
    "                loss = F.cross_entropy(logits, targets)\n",
    "            return logits, loss\n",
    "\n",
    "        def generate(self, index, max_new_tokens):\n",
    "            for _ in range(max_new_tokens):\n",
    "                logits, _ = self.forward(index)\n",
    "                logits = logits[:, -1, :]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                index_next = torch.multinomial(probs, num_samples=1)\n",
    "                index = torch.cat((index, index_next), dim=1)\n",
    "            return index\n",
    "\n",
    "    return GptLanguageModel(vocab_size).to(device)\n",
    "\n",
    "# Define data functions\n",
    "def get_random_chunk(split):\n",
    "    filename = \"train_split.txt\" if split == 'train' else \"val_split.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, file_size - block_size * batch_size)\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size * batch_size - 1)\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "    return data\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix]).to(device)\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix]).to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    return out\n",
    "\n",
    "# Load vocabulary\n",
    "chars = \"\"\n",
    "with open('vocab.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "string_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "# Initialize results\n",
    "best_loss = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Grid search over hyperparameters\n",
    "for params in itertools.product(*hyperparameter_grid.values()):\n",
    "    batch_size, block_size, max_iters, learning_rate, n_embd, n_head, n_layer = params\n",
    "    print(f\"\\nTesting combination: {params}\")\n",
    "    \n",
    "    model = create_model(vocab_size, n_embd, n_head, n_layer)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    skip_combination = False\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "        if iter % eval_iters == 0:\n",
    "            losses = estimate_loss(model, eval_iters)\n",
    "            train_losses.append(losses['train'])\n",
    "            val_losses.append(losses['val'])\n",
    "            print(f\"step: {iter}, train loss: {losses['train']}, val loss: {losses['val']}\")\n",
    "\n",
    "            if iter ==  eval_iters and losses['train'] > 3:\n",
    "                print(\"Skipping to next combination due to high train loss.\")\n",
    "                skip_combination = True\n",
    "                break\n",
    "\n",
    "        xb, yb = get_batch('train')\n",
    "        logits, loss = model.forward(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if skip_combination:\n",
    "        continue\n",
    "\n",
    "    print(f\"Final loss: {loss.item()}\")\n",
    "    \n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        best_params = {\n",
    "            'batch_size': batch_size,\n",
    "            'block_size': block_size,\n",
    "            'max_iters': max_iters,\n",
    "            'learning_rate': learning_rate,\n",
    "            'n_embd': n_embd,\n",
    "            'n_head': n_head,\n",
    "            'n_layer': n_layer\n",
    "        }\n",
    "\n",
    "# Save best parameters\n",
    "with open('best_params.pkl', 'wb') as f:\n",
    "    pickle.dump(best_params, f)\n",
    "\n",
    "print(f\"Best parameters: {best_params} with loss: {best_loss}\")\n",
    "\n",
    "# Plotting losses\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss') \n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f67b02-946e-4ff7-ae56-49f2f0bdf4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
